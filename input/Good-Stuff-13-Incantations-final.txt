00:00-00:02
That's what we did.

00:02-00:05
It was the very first thing I tried to actively start.

00:05-00:08
Pete and I talked about this at Bung's Shot Card,

00:08-00:12
that we both kind of needed actual projects

00:12-00:14
in order to properly learn how to code,

00:14-00:16
rather than trying to learn in the abstracts,

00:16-00:18
just trying to work through and navigate the syntax.

00:18-00:21
You just work with a project that you've got.

00:21-00:24
And yeah, this was the first thing I

00:24-00:28
wanted to have an excuse to play with cursor.

00:28-00:30
Cursor a little.

00:30-00:31
You're digging cursor?

00:31-00:36
Um, I have hit record, by the way.

00:36-00:37
So this is the good stuff.

00:37-00:39
Well, let's start with the intro.

00:39-00:41
Yeah, that'll do.

00:41-00:42
That's usually enough.

00:42-00:44
As you know, it's pretty informal.

00:44-00:49
Over here, it's just chat over coffee.

00:49-00:52
We've got a guest, second guest, Pete.

00:52-00:54
It's UME plus GAV.

00:54-00:55
GAV Fielding.

00:55-00:56
Hey, folks.

00:56-00:57
This is cool.

00:57-01:01
So I don't know if we want to do a little intro for you.

01:01-01:03
I was going to carry on with the cursor chat.

01:03-01:03
Oh, we can't.

01:03-01:04
I mean, we can.

01:04-01:06
I just thought, let's go on a week.

01:06-01:07
I will get around to it.

01:07-01:08
I'll get around to it?

01:08-01:08
OK.

01:08-01:11
Yeah, I mean, it's obviously controversial timing now,

01:11-01:14
because I messed around with their billing last week.

01:14-01:18
So it's about half as effective for it as it is,

01:18-01:19
as it was the week prior.

01:19-01:20
They did, or you did?

01:20-01:21
They did, yeah.

01:21-01:25
So they gave everybody, like, oh, you pay us, what,

01:25-01:28
like $100, $200, and you get an unlimited amount.

01:28-01:28
Unlimited, yeah.

01:28-01:31
And then the unlimited amount, it was like asterisks,

01:31-01:33
like reasonable currencies.

01:33-01:34
And reasonable currencies turned out

01:34-01:36
to be like nothing, and people were just

01:36-01:37
blowing for it in a day.

01:37-01:40
I mean, this was my experience of using cursor.

01:40-01:43
Like, about a year ago, I couldn't use it for more

01:43-01:46
than a couple of days without just blowing, like, all

01:46-01:48
of them, like, the good models.

01:48-01:50
I wasn't allowed them anymore, even though I was paying for them

01:50-01:52
like pretty early on.

01:52-01:52
And it was a good deal.

01:52-01:55
So it was like $20, and you were supposed to get, like, as

01:55-01:59
much Claude Sonnet, $3.5, whatever it was at the time,

01:59-02:00
as you could eat.

02:00-02:01
But you just couldn't.

02:01-02:04
So I just moved to just different apps where I could

02:04-02:08
just pay as I go, which were hideously expensive in terms

02:08-02:12
of, like, the costs of you pay for your API access.

02:12-02:15
But compared to actually just working with another human,

02:15-02:17
it was remarkably cheap.

02:17-02:20
So I was like, well, this is just cheaper than working

02:20-02:21
with somebody else.

02:21-02:22
Yeah, you still kind of know what you're doing, though.

02:22-02:23
Yeah.

02:23-02:25
The question I'd have for you, though, is were you

02:25-02:27
vibe coding or are you slow coding?

02:27-02:29
I was pretty slow coding.

02:29-02:29
Were you?

02:29-02:30
Yeah, I was surprised.

02:30-02:31
I was surprised the other day.

02:31-02:33
It's only what I call regular coding now.

02:33-02:35
That's what we've started calling.

02:35-02:38
I refer to slow coding as you're still working with AI's,

02:38-02:41
and you're still moving, like, very fast.

02:41-02:43
But you're actually looking at the code.

02:43-02:44
Yeah.

02:44-02:45
Well, I would even take a step forward.

02:45-02:49
Like, you're planning out the entire project with one of

02:49-02:52
the better thinking models, and then you're moving into the

02:52-02:55
agent mode and cursor once you've got the whole thing

02:55-02:56
scoped out.

02:56-02:58
And I think that-- and you still do it really quick.

02:58-03:01
It's funny, like, the examples I give to Pete every so

03:01-03:04
often is, I'll plan out a project, get a PRD written up,

03:04-03:08
and Claude will be like, yeah, this will take 12 weeks.

03:08-03:11
I'm like, lol, see you in 10 minutes.

03:11-03:15
That's what-- like, I find that, in general, with anything

03:15-03:18
that I'm doing is actually, like, you still go through all

03:18-03:22
the processes of, like, the thinking bit, the unpacking

03:22-03:26
bit, the designing bit, the, like, what's the actual project

03:26-03:28
plan, what are the steps, who are the people you need to use,

03:28-03:29
all that sort of stuff.

03:29-03:30
Yeah.

03:30-03:33
And then you build that out, and then you do each one in chunks.

03:33-03:33
Yeah.

03:33-03:36
We talked about this a bit with, like, the move of software

03:36-03:39
development that went from, like, waterfall systems into

03:39-03:42
agile, like, 20 years ago, whatever.

03:42-03:44
And then agile has been the dominant thing.

03:44-03:48
And it's like, well, what's the new version of that with AI?

03:48-03:51
And it's not really agile anymore.

03:51-03:53
It's more that you actually reinforce all the stuff you

03:53-03:56
used to do in waterfall to do it correctly, becomes really

03:56-03:57
important.

03:57-03:59
And we're kind of going back to what we call, like, super

03:59-04:03
fast waterfall, which is you go through the entire process,

04:03-04:06
but you do it with a set of AI agents that help you along the

04:06-04:08
way, and you do it very quickly.

04:08-04:11
So, like, whereas one, you know, like, waterfall project

04:11-04:13
might have taken you nine months previously, now you do

04:13-04:15
the whole thing in about a week.

04:15-04:18
If that will break into smaller iterations, you do it in,

04:18-04:20
like, a day.

04:20-04:23
And the limiting factor is the context window of whatever

04:23-04:25
the tool is that you're using.

04:25-04:27
There's probably more in the context prep, I think.

04:27-04:31
I'm not such a big fan of context windows.

04:31-04:35
Because the problem with, like, the big context window is,

04:35-04:41
like, the hammer to screw in a nail, which is, like, just

04:41-04:42
mixed several metaphors there.

04:42-04:44
Like, you know, screw in nails, beat.

04:44-04:45
But we went through that one.

04:45-04:47
We've had that last week, yeah.

04:47-04:48
[LAUGHTER]

04:48-04:50
Yeah, I don't like having to get into that one.

04:50-04:52
Hammer to screw in a screw.

04:52-04:53
You know, like, there's the wrong tool.

04:53-04:54
I'd love to see some of your home furniture.

04:54-04:56
My home furniture, it's been pretty good.

04:56-04:58
I'm making my own furniture these days.

04:58-05:00
I've moved from just, like, hammering together stuff to

05:00-05:04
the van for actually producing well-done furniture.

05:04-05:07
I'm quite proud of myself.

05:07-05:10
What I was saying, like, the context window is you just,

05:10-05:13
like, it's like throwing in more shit and then hoping that

05:13-05:17
the LLM will, like, figure out what's the stuff I need to know

05:17-05:19
inside this context window.

05:19-05:22
Which really, what you actually want is absolutely the minimal

05:22-05:24
amount of context that you can get away with,

05:24-05:26
but only the right things.

05:26-05:28
And it's just, like, that's, to me, like, that's the point of

05:28-05:32
the whole Waterfall process, is to get to that point where you

05:32-05:35
go, like, the bit that you need to know for this task is,

05:35-05:37
like, this.

05:37-05:37
Yeah.

05:37-05:40
I think that's two different things.

05:40-05:41
Yeah.

05:41-05:43
Because there is that kind of, and this is the sort of

05:43-05:46
conversations I've been having, what I even notice, like,

05:46-05:50
back in the pre-AI world, which I think we've had a chat

05:50-05:53
about before, is, like, some people just don't realize the

05:53-05:57
amount of, like, context that they have even just when

05:57-06:01
they're communicating with, like, where in the same city, we

06:01-06:05
speak the same language, we went to the same school, we went

06:05-06:06
to lower that sort of thing.

06:06-06:07
Right.

06:07-06:11
We went to one school in Burr, and everybody went there.

06:11-06:14
At least for a little while.

06:14-06:16
But all that, like, same culture, same, like, all that sort

06:16-06:20
of stuff, down to, like, where languages came from, and all

06:20-06:24
that sort of, like, all that is context that we just have when

06:24-06:27
we talk in a conversation, or where if you're a manager

06:27-06:30
having a conversation with the person that's running your

06:30-06:33
team, and you need to get them to do something, but you

06:33-06:36
don't have that context with an, like, all the intelligence

06:36-06:39
and stuff is there, but you've got to actually wrap it in

06:39-06:41
whatever the context is.

06:41-06:43
I agree with the signage.

06:43-06:45
But when, like, when I say the context, we lose the

06:45-06:49
limitation, because I got to a point where, like, I'm using

06:49-06:54
Clawd a lot, and I'd always run out, but it says, like, this

06:54-06:58
chat's too long, and this is, it's like, oh, okay, I've got

06:58-07:01
to chunk this down into smaller steps to then do that bit

07:01-07:03
within the context window and take that and put it into the

07:03-07:06
next one, and do that bit within it and take that and put it

07:06-07:09
to the next one, whereas, like, before, like, what if all

07:09-07:13
stuff is just, like, it's almost like the time, hey,

07:13-07:14
business needs to be done by then.

07:14-07:17
I find that it's a lot easier to navigate now with projects.

07:17-07:21
I think, like, I'm way less likely to hit limits by using

07:21-07:24
projects, because it also means that I can get a course, a

07:24-07:29
course of knowledge or context popped into the project, and I

07:29-07:33
can go off different tangents in individualized chats, so the

07:33-07:37
chat itself is more structured, and then you can

07:37-07:40
- The projects get, depends what it is, right?

07:40-07:44
- Like, as, I'm working on one at the moment, and after you get

07:44-07:47
too many chats in it, it starts to fall apart, yeah.

07:47-07:49
- I think the other thing there is, like, you do need to be a

07:49-07:53
little bit deliberate and manual about how you prune the

07:53-07:56
information you're extracting from the various chats and then

07:56-08:00
baking that back into the project knowledge as well, which is

08:00-08:06
something that I do a lot of, so I'll basically get key

08:06-08:09
summaries of things that I've highlighted that are important

08:09-08:14
in the chat, get that in markdown, pop that back into the

08:14-08:17
project knowledge, and I find then that when I go into a new

08:17-08:20
chat, then that's just easier to navigate, and I almost never

08:20-08:22
hit limits doing it that way.

08:22-08:25
But I know, before they introduced that, it was the

08:25-08:28
same experience for me, like, I would just constantly hit

08:28-08:30
those limits would be very frustrating, because then you're

08:30-08:33
like, okay, write me a prompt that summaries is everything

08:33-08:35
we've talked about so I can start this in a new chat, and I

08:35-08:38
just never do it as effectively as you wanted it.

08:38-08:40
- And then you track it back.

08:40-08:42
- It was basically, I'm the one bit that I needed to

08:42-08:44
update this bit.

08:44-08:47
- So this reminds me a lot of, like, the conversation we were

08:47-08:49
having with Bill, when we were looking at, like, how do you

08:49-08:53
create, like, a digital twin or a clone of a person, and

08:53-08:55
their knowledge and their knowledge base.

08:55-08:57
And we're saying, like, a lot of the ways that people will

08:57-08:59
go about this is they'll say, like, okay, well, I'm going to

08:59-09:01
create, like, a chat bot for my company.

09:01-09:03
But what I'm going to do is I'm going to write, I'm going to

09:03-09:06
get, like, a bunch of documents, and I'm going to put them in

09:06-09:09
the thing, and then I'm going to ask questions about it, and

09:09-09:12
it's going to be, it's going to answer as though it knows

09:12-09:13
about the company.

09:13-09:16
And it's almost like half the job, if you know what I mean,

09:16-09:19
because what you've done is you've done the bit of, like,

09:19-09:23
you know, if Gab knows a bunch of stuff, and I can, I can

09:23-09:26
be a bit like Gab by knowing the same stuff, and I might

09:26-09:29
answer some questions factually the same way, because I'm

09:29-09:31
pulling from the same, like, rag database or whatever it is.

09:31-09:33
It's in the background.

09:33-09:35
But what I've not done is any of the stuff that you mentioned

09:35-09:37
a moment ago, which is, like, you know, where are you from?

09:37-09:38
What's your confidence?

09:38-09:39
Like, what language are you in?

09:39-09:40
What are your norms?

09:40-09:41
How do you talk?

09:41-09:43
And all of this sort of stuff.

09:43-09:45
And that separates up nicely into, like, there's the

09:45-09:48
things you know and there's the things you are.

09:48-09:51
And I don't think what, like, the bit that's not as well

09:51-09:55
explored, I think, in the AI space is the, the programmatic,

09:55-09:56
the thing you are piece.

09:56-09:57
Yeah.

09:57-10:00
Which is, like, how do I, how do I look through somebody's,

10:00-10:04
like, life work and then distill that down into who is the

10:04-10:05
thing that they are?

10:05-10:05
Yeah.

10:05-10:06
And how's that?

10:06-10:11
How, it's almost like an underlying layer of context

10:11-10:15
within then the top layer, surface layer of stuff, which

10:15-10:16
is the chat.

10:16-10:18
It's like, you know, who is this person?

10:18-10:20
Like, what are their mission at the moment right now?

10:20-10:20
Yeah.

10:20-10:22
What are their values?

10:22-10:23
And, like, what do they believe in?

10:23-10:24
Like, how do they communicate?

10:24-10:26
And you just wouldn't, you can do this manually.

10:26-10:29
Like, you can, like, your first chat with Claude, you can

10:29-10:31
go, right, Claude, you're this, you're going to be a bit

10:31-10:33
like this, you're going to act in this way, your values are

10:33-10:36
this, you do this, this, this, right here are all your core

10:36-10:37
things.

10:37-10:37
Yeah.

10:37-10:40
And then it gets a bit like, but I'm not going to do that

10:40-10:41
every time.

10:41-10:44
And then you get into, like, like, you know, I'll do a

10:44-10:45
project and I'll have a system prompt in there and I'll

10:45-10:47
put everything there.

10:47-10:49
And that makes, and you see the differences.

10:49-10:52
It'll act a lot better at this point in time.

10:52-10:53
And then because it's always got that every time you start

10:53-10:57
a new chat, it has a thing that it is plus some things that

10:57-10:57
it knows.

10:57-10:59
So you've got the two different components.

10:59-11:00
Yeah.

11:00-11:03
And I think the bigger luck is probably that there's so much

11:03-11:06
context that goes into, like, you're on a real, like, human

11:06-11:11
experience is I think that we're not really one mind,

11:11-11:12
necessarily.

11:12-11:14
I think we're multiple different sort of, like, minds or

11:14-11:17
people in a body and we flip them all the time depending on

11:17-11:19
our situation.

11:19-11:20
There's an interesting--

11:20-11:21
Because you see this, right?

11:21-11:23
You go home, like, really pissed off from work and you walk

11:23-11:26
into, like, the kids and you're still being, like, pissed

11:26-11:28
off from work, you guy.

11:28-11:30
And it's, like, entirely inappropriate.

11:30-11:32
And you have to almost, like, go out, like, walk around

11:32-11:33
and walk.

11:33-11:33
Decompress them.

11:33-11:36
Like, just completely change context in some way.

11:36-11:38
Like, this is a problem with working from home is, like,

11:38-11:41
you don't get that context switch and decompress on the

11:41-11:41
way home.

11:41-11:42
Yeah.

11:42-11:44
But you need to get into, like, oh, I'm the other person.

11:44-11:45
Yeah.

11:45-11:45
Yeah.

11:45-11:47
It's a good question.

11:47-11:51
Who's the dude who wrote that?

11:51-11:55
Is it Joshua Bateson or Jonathan Payneson or what?

11:55-11:56
That--

11:56-11:56
Jordan.

11:56-11:57
Jordan.

11:57-11:57
Jordan Payneson.

11:57-11:58
Yeah.

11:58-12:01
Always wears, like, three-piece or that sort of stuff.

12:01-12:01
He--

12:01-12:02
Crawl Falls for work.

12:02-12:03
Has a-- yeah.

12:03-12:07
He talks a lot about, like, different personalities and

12:07-12:08
all that sort of stuff.

12:08-12:11
Oh, listen, in the podcast recently, he's essentially

12:11-12:13
exactly that.

12:13-12:14
I'm sure he did a worse job.

12:14-12:17
It seems to me that he's arguing about definitions and

12:17-12:19
not really being useful.

12:19-12:20
Use many more words.

12:20-12:22
It's so annoying.

12:22-12:24
Many big words to describe something simple.

12:24-12:26
I think, like, the other thing that occurs to me with

12:26-12:30
that chat is that it's a pretty opaque thing for other

12:30-12:32
humans when they're interacting with humans, right?

12:32-12:36
I mean, like, there's who you say you are.

12:36-12:38
It's who you think you are.

12:38-12:39
There's who you actually are.

12:39-12:41
I mean, on those levels-- and those things are constantly

12:41-12:43
changing, as Pete mentioned as well--

12:43-12:46
it's really hard for anyone to really have, like, that really

12:46-12:48
clear context when they're dealing with another human.

12:48-12:51
I think that's one of the challenges of just

12:51-12:53
interoperating with other humans.

12:53-12:53
Like, that's--

12:53-12:57
That's interesting, though, like, but we've got Vision and

12:57-12:57
hearing.

12:57-13:00
We pick up, like, body language cues, all that sort of

13:00-13:01
stuff.

13:01-13:04
How does this play out when, like, that's why Zucks has--

13:04-13:07
he's drawn all these, like, glasses and doing his--

13:07-13:07
Oh, yeah.

13:07-13:09
--like, things with Ray-Bans and all that.

13:09-13:16
And Musk has Teslas flying around, capturing Vision

13:16-13:18
everywhere.

13:18-13:26
I think it was the head of AI of Meta said, like, all the

13:26-13:30
written down words and information in the world

13:30-13:33
has been consumed and learned with, like, they've learned

13:33-13:35
it all in the ALLMs and that sort of stuff.

13:35-13:36
And that equates to the same amount--

13:36-13:37
That's so eschewed, right?

13:37-13:39
Yeah.

13:39-13:41
We've done right in.

13:41-13:42
Yeah.

13:42-13:48
But it amounts to, you might know, like, the, like, bits

13:48-13:53
and, like, the kind of, like, information, and all that

13:53-13:54
bit all.

13:54-13:58
But it's, like, it equates to four years of human life

13:58-14:01
and the amount of information that goes through, like,

14:01-14:03
the visual cortex and what they hear and all that sort

14:03-14:04
of stuff.

14:04-14:06
Well, you can, like, smell or something like that, right?

14:06-14:07
Yeah.

14:07-14:09
There's all this stuff we don't even notice is coming on.

14:09-14:10
Yeah.

14:10-14:16
But there'll be a point where that stuff is just, like,

14:16-14:19
there'll be sensors and there'll be humanoid robot

14:19-14:21
butlers walking through and, like, picking up all those

14:21-14:23
cues and all that sort of stuff.

14:23-14:26
And what it might guess is that, like, we would look at,

14:26-14:29
say, like a-- we took it with a robot butler and we'd say,

14:29-14:31
like, well, what is this robot butler doing?

14:31-14:36
Like, I don't think it would be one sort of agent that is,

14:36-14:38
like, the robot butler.

14:38-14:41
It would probably-- my guess is that, like, we'll start to

14:41-14:44
learn more about maybe how human minds work a little bit,

14:44-14:47
which is that we just have, like, all of these different

14:47-14:50
people and contexts and things that we can draw on

14:50-14:53
inside ourselves and we just pick them at runtime

14:53-14:55
for what's appropriate to this situation.

14:55-14:58
So, you know, I'm not being dancing Pete right now

14:58-15:00
because I have been in this room.

15:00-15:03
[LAUGHTER]

15:03-15:05
So, you know, this is not the mood we're going for, right?

15:05-15:08
At the moment, I'm, you know, sitting back and pretending

15:08-15:09
to be think boy Pete.

15:09-15:11
[LAUGHTER]

15:11-15:14
But, like, you'd-- like, you choose, like, which parts

15:14-15:17
of yourselves and, like, which context that you want to bring

15:17-15:17
up.

15:17-15:20
But if you bring too much to it, then it gets confusing

15:20-15:22
and, like, you know what's going on.

15:22-15:25
I think you'd see the same thing in the eyes where, again,

15:25-15:28
like, you need the context for-- like, the context window

15:28-15:31
of the who you are and the what you know both need to be

15:31-15:33
appropriate for this situation.

15:33-15:36
And so you almost have this, like, multi-level filtering

15:36-15:38
of going, like, what just happened?

15:38-15:39
Like, how do I try?

15:39-15:40
What's appropriate for this?

15:40-15:43
And you're, like, you filter down to effectively getting

15:43-15:45
into, like, one little agent that goes, like, OK, here's

15:45-15:46
everything that's going on right now.

15:46-15:48
Here's exactly how you need to be to respond to this

15:48-15:50
and then you respond.

15:50-15:52
And that could have been, like, a thousand different, like,

15:52-15:55
computations and LLN calls in the background.

15:55-15:57
But to us, it'll look like we just said one thing

15:57-15:59
and it did one thing in response.

15:59-15:59
Mm-hmm.

15:59-16:02
Yeah, but it's not dissimilar to how most humans

16:02-16:03
understand another human.

16:03-16:06
Like, only very few people would know, you know,

16:06-16:08
all of the contexts of Pete, right?

16:08-16:10
Like, the family, very close friends.

16:10-16:12
It's the same for all of us.

16:12-16:15
The person you might bump into down the street once a week

16:15-16:17
or the person you might buy you coffee from.

16:17-16:18
They get a version of you.

16:18-16:20
That's their understanding of who you are.

16:20-16:20
That's it.

16:20-16:22
It's not multi-dimensional.

16:22-16:26
So it'd be quite similar to the vast majority of human

16:26-16:28
interactions in that respect, I imagine.

16:28-16:31
We also don't necessarily call in response as well, right?

16:31-16:33
Because I'm an absolute bugger for this.

16:33-16:33
I'll admit it.

16:33-16:35
But, like, when other people are talking,

16:35-16:39
like, I have, like, another track going that's just trying

16:39-16:42
to figure out, like, ooh, what shall I say next?

16:42-16:44
And I really do try and, like, listen and be present.

16:44-16:47
But it's-- I think everybody just does this.

16:47-16:49
Like, everybody's thinking, like, all right,

16:49-16:49
well, what's that person saying?

16:49-16:50
What do I know?

16:50-16:53
And, like, things are popping up into your head as you go.

16:53-16:54
And all this stuff is getting presented.

16:54-16:57
It just doesn't come out of the mouth yet.

16:57-16:59
I think that's what they call active listening.

16:59-17:02
It's like, you actually-- you need to actually be in the mode

17:02-17:05
of listening and taking it in.

17:05-17:08
I feel like, again, this is kind of interesting, right?

17:08-17:10
From a, like, how do people work point of view?

17:10-17:13
But I often feel like, even when I'm, like, actively listening

17:13-17:17
to stuff as much as I can, like, my mind's quite visual.

17:17-17:20
So I almost, like, see it as, like, the words that I'm

17:20-17:21
hearing getting put into buckets.

17:21-17:23
Yeah.

17:23-17:26
I'm still going to make links between various different things.

17:26-17:28
And other stuff will just pop up into my head.

17:28-17:31
Maybe I just need to meditate more.

17:31-17:32
So, like, the memory thing.

17:32-17:34
Maybe I'm coming across as a dick.

17:34-17:35
You do that thing with the memory thing.

17:35-17:36
I think you just memory--

17:36-17:37
No, you just--

17:37-17:39
Visualize the room, and you put everything in the room

17:39-17:40
to go to the back and remember it.

17:40-17:42
Because everything comes out visually for me.

17:42-17:44
But, like, I find it really hard if I can't.

17:44-17:47
But one of the things that does bug me a little bit about,

17:47-17:50
like, speech-first interfaces for real is,

17:50-17:51
which I think are good.

17:51-17:53
But in my head, like, when I communicate through speech,

17:53-17:56
like, I've got a picture that I'm describing to somebody else,

17:56-17:58
and I assume they're trying to recreate a picture in that.

17:58-18:01
But the whole speech medium feels very lossy.

18:01-18:05
So I'm always quite quick to move to whiteboards or something.

18:05-18:06
Yeah, yeah.

18:06-18:07
Yeah.

18:07-18:12
Well, that's, like, your learning styles and that type of thing.

18:12-18:15
Which I think is going to be both big and then both,

18:15-18:19
like, game changer as far as, like, the--

18:19-18:21
Like, I've got kids on that.

18:21-18:24
I forget the name of it, but it's, like, an AI.

18:24-18:25
Math-tuning.

18:25-18:25
Oh, yeah.

18:25-18:26
Yeah.

18:26-18:29
And I mean, like, I was sitting there for ages,

18:29-18:33
and I was, and I finally, like, got to see it, try this.

18:33-18:39
Straight away, he went from, like, wanting to play video games.

18:39-18:41
I do this, I'm not going to just--

18:41-18:45
And it just engages, and then it presents in an ass context.

18:45-18:49
And then it redesigns the kind of, like, lesson plan,

18:49-18:54
or the, like, the interaction to have the context of what--

18:54-18:59
And they're just, like, altering age, learning it.

18:59-19:01
Like, emails you were--

19:01-19:03
what they did in a session, and what they learned,

19:03-19:05
and what they struggled with, and all that sort of stuff.

19:05-19:07
Epic, as a parent.

19:07-19:12
But it's understanding the best ways to communicate.

19:12-19:14
Yeah.

19:14-19:19
Like, some of the stuff, like, with the going out to businesses

19:19-19:26
and that I want to actually start to do intro-ish side

19:26-19:28
of diagnosis for the business, but then get each person

19:28-19:30
to do, like, a little mini-personality test,

19:30-19:32
or how best do I learn, and all that sort of stuff.

19:32-19:36
And then that goes into the context to then be, oh,

19:36-19:41
so this is the output of whatever part of the project it is.

19:41-19:42
Here's how we have to communicate to this person,

19:42-19:45
and to this person, and to this person.

19:45-19:49
And then, like, that, in turn, can become part of, like,

19:49-19:52
that helps with the organizational change piece,

19:52-19:55
and all that sort of stuff that's coming up.

19:55-19:58
But that sort of stuff is, like, it's really important.

19:58-20:00
I've found even just, like, managing teams,

20:00-20:06
it's almost as important, like, not even necessarily

20:06-20:10
knowing the differences, but having people

20:10-20:12
know that there are differences.

20:12-20:16
And they go through the process, and they, like, oh,

20:16-20:17
we're all different.

20:17-20:19
And then all of a sudden, their mind changes.

20:19-20:20
Yeah.

20:20-20:22
I've done this in the past by a bunch

20:22-20:26
with, like, large program rollouts over here,

20:26-20:27
mining companies, and stuff like that.

20:27-20:31
We do a lot of psychological profiling of people,

20:31-20:33
and, like, what their stress states are,

20:33-20:37
and non-stress states, and just sharing that amongst the team,

20:37-20:38
and getting them to realize that, like, oh, shit,

20:38-20:41
like, that's why he's acting like this.

20:41-20:43
It's like, you can see it under the surface all of a sudden.

20:43-20:45
People become a bit more forgiving of each other,

20:45-20:46
give them space.

20:46-20:49
But I think that's quite interesting in an AI context

20:49-20:51
as well, we can see it, but yeah.

20:51-20:52
Well, yeah, like, and it's--

20:52-20:54
--get the information across.

20:54-20:57
And also, because, like, with the voice things, I find them--

20:57-20:59
they're getting quite good, but they're still bloody annoying,

20:59-21:02
because they keep cutting me off when I'm trying to talk to it.

21:02-21:03
It's not quite natural.

21:03-21:05
It can't quite make that decision.

21:05-21:08
And like, just wait until I finish and say, OK, order.

21:08-21:09
And it just cuts you off.

21:09-21:12
And then it completely ruins the conversational aspect.

21:12-21:15
I wonder how much of it is the lack of visual cues and stuff

21:15-21:16
like that.

21:16-21:17
If you think about--

21:17-21:19
you get a worse conversation with somebody over a telephone

21:19-21:23
than you do at a video conference than you do in person.

21:23-21:27
And like, most of the time, if I'm talking to AI,

21:27-21:31
I'm out walking around in nature, and it's in my pocket.

21:31-21:34
And like, if I asked Andy to talk to me from my pocket,

21:34-21:36
he'd probably be like, it's not very unique.

21:36-21:38
I just said some work.

21:38-21:39
There's a whole--

21:39-21:41
I'm sure he's just lying on of me.

21:41-21:43
[INTERPOSING VOICES]

21:43-21:45
I could pull some threads on that, but we won't.

21:45-21:48
[LAUGHTER]

21:48-21:48
That's true, though.

21:48-21:49
It's fine.

21:49-21:51
It's weird.

21:51-21:51
I don't know.

21:51-21:53
Does Grock have a voice?

21:53-21:54
Yeah, it does.

21:54-21:55
Yeah, I think they made it.

21:55-21:57
Yeah, as usual, it was all gimmicky.

21:57-21:58
It was kind of like a--

21:58-21:59
Yeah, it's a sexy voice.

21:59-22:00
Be a--

22:00-22:03
Yeah, off the rails or whatever thing.

22:03-22:06
Because you know how Mustas lose like epic long pauses?

22:06-22:07
Yeah.

22:07-22:09
In conversation when he's like thinking of the answer

22:09-22:10
and stuff like that.

22:10-22:12
It's like a thing you have to listen to.

22:12-22:15
He can't train the thing to respond to him.

22:15-22:16
I think it's--

22:16-22:18
Have you tried Sesame?

22:18-22:20
Like, they've only got a demo, I think.

22:20-22:21
I've not actually--

22:21-22:22
Oh, I've heard--

22:22-22:23
Yes.

22:23-22:23
It's like--

22:23-22:24
I think I've told people about it.

22:24-22:28
It's like, I think it's the closest thing to like the movie

22:28-22:31
her that I've come across from boys.

22:31-22:34
And I think the key is it will still cut you off.

22:34-22:35
I mean, the demo is only short.

22:35-22:38
15 minutes.

22:38-22:41
Is this one of those like AI boyfriend/girlfriend--

22:41-22:43
No, no, it's just a voicemall.

22:43-22:43
It's just a boy.

22:43-22:45
It's like an 11-lebsy.

22:45-22:48
It's not like pre-fund.

22:48-22:49
It's just general purpose.

22:49-22:55
But it's the most natural sounding voice that I've tried.

22:55-22:58
But I think it's like less prone to cut you off

22:58-22:59
if there's like a latency issue.

22:59-23:03
Like if you're taking a short break--

23:03-23:05
because we do this in natural speech.

23:05-23:08
We'll take momentary pauses.

23:08-23:10
And I think the visual cue is definitely

23:10-23:11
an aspect of that.

23:11-23:13
But I think they just elongated it out a little bit.

23:13-23:16
So if there's a pause that's only like a half second

23:16-23:20
or something, we're not going to cut you off.

23:20-23:25
I'd say it'll be solved when we're all walking around with AI

23:25-23:28
clips or whatever, Johnny Ives and Sam Oggman bring out next.

23:28-23:29
I'll die of it.

23:29-23:30
They are--

23:30-23:31
Not that one.

23:31-23:32
We'll try and avoid that one.

23:32-23:35
You know, just to your point about learning, though,

23:35-23:39
like you know ChatChimp Potatoe just released a Tudor mode.

23:39-23:40
Oh, did they?

23:40-23:40
Yeah.

23:40-23:42
I think it's-- and it basically is--

23:42-23:43
I haven't tried it.

23:43-23:46
But it's basically just designed to, instead of giving you

23:46-23:48
the answer straight away, it's meant to walk you through

23:48-23:52
like a process to figure out how you might best

23:52-23:54
arrive at that answer yourself.

23:54-23:55
Which I think is interesting.

23:55-23:57
But again, I don't necessarily think

23:57-23:59
it's tailored to your learning style

23:59-24:01
because this is not going to know.

24:01-24:02
Yeah.

24:02-24:05
But then be like an intro and onboarding thing shortly

24:05-24:07
and that--

24:07-24:09
I think over time-- maybe it's a memory thing, right?

24:09-24:12
Like the more it gets to know you over time.

24:12-24:12
Yeah.

24:12-24:12
But--

24:12-24:15
See that MIT study, I think it was MIT.

24:15-24:22
It was one of like looking at the brain activity of people

24:22-24:27
using like ChatChimp Potatoe to respond to things and not--

24:27-24:28
It's fascinating, isn't it?

24:28-24:28
Yeah.

24:28-24:29
I reckon it says more about humans

24:29-24:31
than it does about AI.

24:31-24:35
Well, this is not--

24:35-24:37
It's people who become passive and all that stuff.

24:37-24:37
But general--

24:37-24:38
--directions.

24:38-24:38
--no matter what.

24:38-24:39
Right.

24:39-24:40
You remember, I don't--

24:40-24:42
well, actually, I'm talking to two people from person.

24:42-24:43
This might not work.

24:43-24:48
But your directions are just go forwards for 400Ks, right?

24:48-24:50
And then the left.

24:50-24:50
The left.

24:50-24:51
The first one.

24:51-24:55
It's like-- it's like you would get like two--

24:55-24:58
when I was young, you'd get two dads together in the UK.

24:58-25:00
And they'd say to me like, oh, way off to.

25:00-25:05
And they'd say, oh, I'm going from London to--

25:05-25:06
I don't know.

25:06-25:07
I'll pick some Coventry.

25:07-25:09
And you'd be like, all right, you can take the M1 or the A1.

25:09-25:10
You're going to do the A1.

25:10-25:11
You can skip off onto the M69.

25:11-25:15
And then that takes you by the bypass and up through the A1

25:15-25:17
MM44924.

25:17-25:20
And they'd be like, no, like all of the little back roads.

25:20-25:22
Like all the little places that you go.

25:22-25:24
You know, like which places have got traffic at what time.

25:24-25:26
And it was obviously like this mental sparring

25:26-25:28
competition between the two people.

25:28-25:31
And then these days, it's just like,

25:31-25:34
I'm going to just press the button in the screen.

25:34-25:35
And it will tell me where to go.

25:35-25:38
And I'm like, that knowledge is just--

25:38-25:38
You've lost it.

25:38-25:39
Go on.

25:39-25:39
Yeah.

25:39-25:40
Google Maps killed it.

25:40-25:41
Yeah.

25:41-25:43
Even when I was a kid, when I first started to drive,

25:43-25:44
I was using--

25:44-25:47
having to like print out maps and directions

25:47-25:50
and then have all the road numbers on a little steep.

25:50-25:51
I'm going somewhere I completely

25:51-25:52
didn't know any of the roads.

25:52-25:52
I do my ride.

25:52-25:55
Well, that road goes to that road, goes to that road.

25:55-25:58
And you start to realize the logic of the naming bit.

25:58-26:00
Like, you know, the A1--

26:00-26:02
like the A14 comes off the A1.

26:02-26:04
And that's why it's a 1-4.

26:04-26:04
And stuff like that.

26:04-26:06
And I was like, there's a maze.

26:06-26:06
And I painted the room.

26:06-26:09
And I was like, huh, that makes loads of sense.

26:09-26:10
Never picked that up in the numbering.

26:10-26:11
Well, again, you lived in Perth.

26:11-26:12
It's not.

26:12-26:13
You got road one.

26:13-26:15
You got road two.

26:15-26:17
But even when we were younger and we had like the map book

26:17-26:20
in the car, you'd have to like stop at the side of the road

26:20-26:21
and like flip through it.

26:21-26:23
I remember delivering pizzas.

26:23-26:24
Oh, yeah.

26:24-26:27
And having a little code like--

26:27-26:30
so in the shop, it's like, oh, so here's the address.

26:30-26:34
OK, I need to-- and having my little code of like two, right,

26:34-26:36
then like three, then it's like left.

26:36-26:40
And then on the back of the delivery switch.

26:40-26:43
But it was also cool because there was like discovery baked

26:43-26:44
into it as well.

26:44-26:45
Like, you could get lost, but you also

26:45-26:48
couldn't really get lost in Perth.

26:48-26:50
So there was this process of discovery.

26:50-26:51
Every time you got behind the wheel,

26:51-26:54
you were driving, exploring, seeing stuff

26:54-26:55
that was new.

26:55-26:56
Yeah.

26:56-26:57
How's the arm?

26:57-27:02
Because I've noticed that like just like with parents

27:02-27:04
and my in-laws and stuff.

27:04-27:08
And it's just they still want to describe how to get to somewhere.

27:08-27:09
Like, oh, I've got to go here.

27:09-27:10
Oh, are you going to go this way or this way?

27:10-27:12
And I'm just like, I'm just going to get in the car

27:12-27:13
and type in the address.

27:13-27:15
Yeah, my parents do this as well.

27:15-27:18
There's almost like a lag in the updating of the knowledge

27:18-27:21
and the behaviors in general.

27:21-27:25
But it was also probably like driving for way longer

27:25-27:28
in that prior era than they have with Maps.

27:28-27:30
Whereas most of the time that we've been driving

27:30-27:32
has been with Google Maps.

27:32-27:35
There was that like small period before.

27:35-27:37
I was like maybe a few years.

27:37-27:43
But like the recency is just like completely overshown that.

27:43-27:45
Whereas for them, it's like they would have had 20, 30 years

27:45-27:48
of driving.

27:48-27:51
This is interesting though, because I was just

27:51-27:53
listening to a--

27:53-27:56
moonshots, Peter, Diamandies.

27:56-27:57
Yeah, exactly.

27:57-27:58
Like--

27:58-27:59
That's great.

27:59-28:00
Yeah.

28:00-28:04
And they reckon that there'll be like a century of progress

28:04-28:06
in like a decade.

28:06-28:06
Do you know what--

28:06-28:09
Like the way AI is going, it's like the adaptability.

28:09-28:10
They're probably well.

28:10-28:11
Yeah, like the adaptability.

28:11-28:15
And like Dave Sinclair's got--

28:15-28:19
it's like doubling lifespan and health span

28:19-28:22
within the next decade or something,

28:22-28:23
like that sort of stuff.

28:23-28:25
I mean, that's just epic.

28:25-28:29
I think just the acceleration of software development

28:29-28:33
and what you can get done and for how little costs.

28:33-28:37
And it just unlocks so many like highly personalized use

28:37-28:40
cases that you just would never have liked that previously.

28:40-28:45
Like you can custom build like AI bots for your own health

28:45-28:48
that are going to give you questions that--

28:48-28:51
and it's in an area where you almost can't sell that.

28:51-28:55
Like the restrictions around what you could do that would

28:55-28:58
potentially give somebody some health advice based

28:58-29:02
on all of their data would be so bad that you couldn't

29:02-29:03
commercialize that as a product.

29:03-29:05
But somebody can just write that open source

29:05-29:08
or just tell you what to do and you build it in Claude.

29:08-29:09
Yeah.

29:09-29:10
You get--

29:10-29:11
I'm trying-- after you saying that now,

29:11-29:14
I'm trying to like rebuild how you're thinking of it

29:14-29:16
in my head like you were saying before.

29:16-29:17
There we go.

29:17-29:18
We've been safe.

29:18-29:19
All right.

29:19-29:20
About half an hour after we do the intro--

29:20-29:23
We can probably show you that because we've actually

29:23-29:24
built a version of this already.

29:24-29:25
The health thing?

29:25-29:26
Yeah.

29:26-29:26
Have you?

29:26-29:28
I've already done this, yeah.

29:28-29:28
Yeah.

29:28-29:29
Definitely show you that.

29:29-29:31
So we can--

29:31-29:32
That sound.

29:32-29:34
We can go offline and have a look at that.

29:34-29:36
But yeah, I think that's--

29:36-29:38
I think it's like a really fascinating use case for AI.

29:38-29:39
Oh, yeah.

29:39-29:42
I'm meeting a GP man and I'm having--

29:42-29:47
going to have to pick his brain on how to fix that whole thing

29:47-29:49
because I think that there's some fun--

29:49-29:50
I mean, there--

29:50-29:51
--reports and all that works.

29:51-29:53
Like, over half their job is admin.

29:53-29:54
It's paperwork.

29:54-29:55
Right?

29:55-29:58
And so the window of opportunity they've got with the patient

29:58-30:01
is diminishing every year because they have to do more

30:01-30:02
and more paperwork.

30:02-30:04
The insane thing is that the business model is fucked.

30:04-30:04
Yeah.

30:04-30:05
I might.

30:05-30:05
Yeah.

30:05-30:07
There is like a really interesting question,

30:07-30:09
but let's do a quick intro.

30:09-30:12
I feel like we're going to finish the pod.

30:12-30:12
We haven't quite--

30:12-30:16
I'm good at dragging conversations down to the top.

30:16-30:20
Because you've got quite a diverse background as well.

30:20-30:22
So I actually don't know how to introduce you

30:22-30:26
other than to say your GAV, your digital marketing

30:26-30:30
specialist, branding specialist, and maybe the part that I

30:30-30:32
know that maybe not everyone knows

30:32-30:35
is that you're an artist as well.

30:35-30:36
Yeah, sometimes.

30:36-30:38
Sometimes I've got a painting in the garage that's

30:38-30:43
been hanging on the wall half done for like eight months

30:43-30:47
and 12 months that I'll tinker on every so often.

30:47-30:49
So not as much?

30:49-30:49
Not as much now.

30:49-30:51
I've got to get more free time.

30:51-30:53
Man, like, keep--

30:53-30:56
But still, once an artist always an artist.

30:56-30:57
An artist at heart.

30:57-30:59
Yeah.

30:59-31:00
Exploratory.

31:00-31:02
When everyone's got a hoops of time on their hands.

31:02-31:04
In another five years, everyone will be artists.

31:04-31:05
Yeah.

31:05-31:07
But you've obviously been spending a lot of time

31:07-31:12
thinking about AI and thinking about how you can help

31:12-31:14
companies navigate this space, this transition,

31:14-31:17
or what it means for them.

31:17-31:19
Obviously, we've all been having these kind of chats.

31:19-31:22
So it kind of just made sense to just keep having these chats

31:22-31:23
over coffee.

31:23-31:25
Let's keep doing it over coffee, but hit record.

31:25-31:26
Yeah.

31:26-31:29
So that's what we're doing.

31:29-31:33
Yeah, so like, I know that's a pretty good intro.

31:33-31:36
I don't know how much digital I am anymore,

31:36-31:40
but I sort of started out in that creative space,

31:40-31:44
studied design, and advertising, photography,

31:44-31:47
all that sort of stuff.

31:47-31:51
And you get trained how-- and this is sort of the creativity

31:51-31:53
piece-- you get trained and taught

31:53-31:57
how to be more creative, which is a thing, in my opinion,

31:57-31:59
like everyone thinks, creativity is this magical,

31:59-32:02
special thing, but it's just a volumes game.

32:02-32:05
And you have more ideas.

32:05-32:07
Some of them are good, some of them are bad,

32:07-32:08
and you filter them all out.

32:08-32:11
And then all of a sudden, you get the really kind of

32:11-32:14
unpredictable, but really good on point one.

32:14-32:18
Nothing's better doing that than a computer with a volume.

32:18-32:20
You're not going to be able to beat that.

32:20-32:23
So you have a--

32:23-32:25
So Mad Men like to me.

32:25-32:27
I don't just have to drink scotch in the office

32:27-32:28
while having a nap.

32:28-32:30
And then eventually, I don't know when--

32:30-32:33
The idea comes to--

32:33-32:36
That's the lifestyle part, not the creative part.

32:36-32:37
That's the sale.

32:37-32:40
You're still on the lifestyle part.

32:40-32:44
But like I say, it raises the ceiling and lowers the floor.

32:44-32:51
So it lowers the floor of just general generic stuff,

32:51-32:53
and this makes all of that real easy.

32:53-32:57
So you go and you prompt something without much context

32:57-32:59
or whatever, and it gives you something generic back.

33:00-33:03
But if you really know what you're doing,

33:03-33:05
or you are a really good creator,

33:05-33:08
or you are a really good whatever XYZ expert,

33:08-33:13
it actually raises the ceiling as to what you can do.

33:13-33:16
Almost like raises the ceiling to genius.

33:16-33:22
And then that then historically was a scarce thing,

33:22-33:26
because you had one genius spending their time

33:26-33:28
to create one thing.

33:28-33:32
Whereas now you can almost work through and program that

33:32-33:37
and automate that workflow once you crap the code.

33:37-33:41
And then the genius becomes scalable.

33:41-33:46
So it's probably where agencies and stuff are going.

33:46-33:48
But then you lose the scarcity component,

33:48-33:52
so then almost the way that plays out is something else.

33:52-33:53
It sort of reminds me--

33:53-33:55
I can't remember the name of this principle,

33:55-33:58
but it's the one where you assume that something you read

33:58-34:00
in the paper is correct, because you

34:00-34:02
assume it's written by some internet expert.

34:02-34:04
So you see it written in your field.

34:04-34:06
And you go, oh, that's all completely wrong,

34:06-34:08
because I know what I'm talking about here.

34:08-34:10
And then you look at everything else in the newspaper,

34:10-34:11
and you go, oh, shit.

34:11-34:12
Maybe it's all wrong.

34:12-34:14
And I just don't know, because I don't know enough

34:14-34:15
about that field.

34:15-34:17
And I think we have this perception--

34:17-34:20
and it's definitely a thing with creativity,

34:20-34:22
where people would go, that's literally magic.

34:22-34:24
I don't know how you do that.

34:24-34:26
And then under the hood, it's really just a really systematic

34:26-34:30
way of working through lots and lots and lots of small tasks.

34:30-34:33
But each do use creativity and intelligence

34:33-34:34
as part of them.

34:34-34:37
But there's a process for how you do it.

34:37-34:39
In the same way that when you hear people talk

34:39-34:43
that are really good at music or really good at software

34:43-34:45
engineering, they almost always have

34:45-34:48
a structured process for how they're going about it.

34:48-34:52
And what you see is one chunk and one step in that process.

34:52-34:55
So that is actually 20 under the scenes.

34:55-34:57
You just see the output.

34:57-34:58
And the people that are really good

34:58-35:00
are often the ones that understand that,

35:00-35:03
can enumerate it to themselves, and then follow it again.

35:03-35:07
So they get really good retreading of that process

35:07-35:09
and acceleration.

35:09-35:11
There is a really fascinating line here

35:11-35:13
about what you lose with AI.

35:13-35:18
This is the doorman fallacy, where it's like you hyper-focus

35:18-35:22
on a task that you want to optimize, or you want to automate.

35:22-35:25
And you underestimate all of the things

35:25-35:27
that that was providing.

35:27-35:28
It might be the process itself that you

35:28-35:31
have to navigate through that was giving you

35:31-35:34
all of this contextual understanding of your craft.

35:34-35:36
And the doorman is a really good one,

35:36-35:37
because it's like everyone just assumes

35:37-35:38
you're just opening a door for someone

35:38-35:40
as they walk into a building.

35:40-35:42
But it's like all of the intangible stuff

35:42-35:43
that that was doing as well.

35:43-35:45
It's like greeting everyone.

35:45-35:47
It's like a customer service experience for everyone

35:47-35:48
on the way in.

35:48-35:52
And you lose-- like your building rapport

35:52-35:53
with all of these people that you might just

35:53-35:55
see in that brief interaction at the start of the day

35:55-35:59
or the end of the day as they walk in and out of buildings.

35:59-36:03
And so it's like by narrowly defining that role down

36:03-36:07
to its subatomic part, it's like, OK, well, we can automate that.

36:07-36:09
We have electric doors now that just open and shut

36:09-36:11
as they detect your movement.

36:11-36:13
But we've lost all of the intangible stuff.

36:13-36:16
And so that's always an interesting question,

36:16-36:19
which we're kind of leaning into a little bit before the intro,

36:19-36:21
which is like, what do you lose in this process?

36:21-36:24
Well, I think it's a bit like the naive automation

36:24-36:25
of the paint job.

36:25-36:26
Because if you ask the doorman, he

36:26-36:30
knows that his job isn't opening and closing the door.

36:30-36:33
It's knowing and understanding all the different people.

36:33-36:36
Whereas the McKinsey cunts would come in and say,

36:36-36:38
oh, he just opens and closes the door

36:38-36:40
and not think to ask a question that's

36:40-36:43
any deeper than that sort of service level.

36:43-36:46
And therefore, they put in the door opener

36:46-36:47
and have that issue.

36:47-36:50
Whereas I think, as we said at the start of the session here,

36:50-36:55
it's like, what appears to you to be the one thing or the one

36:55-36:58
action that has been going on is just not the whole story.

36:58-37:01
There's layers below that.

37:01-37:04
And I think this is where the AI automation game goes,

37:04-37:06
is that the key is understanding what's

37:06-37:08
all the layers underneath that are going on here?

37:08-37:10
And how do you pick those apart?

37:10-37:12
Because once you get it at that more granular level of like,

37:12-37:14
all right, it wasn't one thing that was going on.

37:14-37:15
That's important.

37:15-37:16
We can do that bit.

37:16-37:18
But there was like 50 other things over here.

37:18-37:21
Let's also get all of that and ultimately using the AI.

37:21-37:24
So that's when you get a really good response out

37:24-37:27
of these things.

37:27-37:28
Yeah.

37:28-37:31
I think this is the risk, though, because I think a lot of people

37:31-37:33
are sat around at the moment pontificating on how best

37:33-37:37
to walk into an organization and rewire it.

37:37-37:40
And it's like, well, we don't know.

37:40-37:43
This is the problem with consultants, right?

37:43-37:45
Is they walk into a business they barely understand,

37:45-37:47
because they might go, well, I've worked with similar types

37:47-37:48
of businesses before.

37:48-37:50
But it's like, but not this one.

37:50-37:52
And this one's run by a group of people

37:52-37:53
who've run it for 25 years.

37:53-37:55
So they just know the internals.

37:55-37:57
They understand the things that work.

37:57-37:59
They understand the things that don't work particularly well.

37:59-38:01
They know the names of their customers, kids.

38:01-38:05
You know, all of this stuff can't really be replicated easily.

38:05-38:08
And I think that's the risk, is if you try to just go right

38:08-38:14
down to the absolute, automatable version of a process,

38:14-38:17
it's like, there is this quantum of what

38:17-38:21
is lost that was still valuable, that needs to be determined.

38:21-38:23
And I feel like that's a really valuable part of the process

38:23-38:25
that everyone's overlooking at the moment.

38:25-38:27
Everyone's just like, look at this workflow that I've just

38:27-38:31
gone and now just spits it out without thinking about what

38:31-38:32
was lost in the process.

38:32-38:34
That might be valuable to retain.

38:34-38:36
I also think this is why personal tools for yourself

38:36-38:37
do a lot better.

38:37-38:37
That's true.

38:37-38:41
Because when you're the expert, just automating your own job,

38:41-38:45
you tend to account for a lot more of this nuance.

38:45-38:49
You see that, this isn't really doing me properly.

38:49-38:51
So it's probably this better that needs to go in.

38:51-38:55
It's not doing this better of what I would usually do.

38:55-38:56
Yeah.

38:56-39:02
I think that almost leans into the purpose problem.

39:02-39:03
You automate a whole heap of stuff.

39:03-39:07
And if you play it all out, I might

39:07-39:10
still have the period domain podcasting my head.

39:10-39:13
But the abundance thing, and all of a sudden,

39:13-39:15
it's like, well, if everything's abundant,

39:15-39:18
then what's the purpose?

39:18-39:20
And then if you roll that back into automating the thing

39:20-39:22
of going into a business, it's like, actually,

39:22-39:28
like what is the purpose of what do you guys want to be doing

39:28-39:29
and what is your purpose?

39:29-39:34
Because if everything automates and knowledge and intelligence

39:34-39:38
is becoming commoditized, so that then shifts.

39:38-39:43
And this is sort of where the stuff I'm doing and targeting

39:43-39:48
more like SMB and traditional service stuff.

39:48-39:52
If that knowledge and intelligence commoditizes,

39:52-39:56
it completely shifts where value comes from.

39:56-40:00
So then it's like, well, how are you going to capture that value?

40:00-40:03
And is it like, did you guys enjoy-- or like,

40:03-40:07
when you started your business, did you enjoy doing this bit

40:07-40:10
that now isn't actually generating much value?

40:10-40:13
And if you shuffle your business around and do this bit,

40:13-40:14
are you going to still enjoy it?

40:14-40:16
Are you still going to be fulfilled with it?

40:16-40:17
And all that sort of stuff.

40:17-40:19
And then the answer to that question

40:19-40:21
completely changes everything.

40:21-40:26
So it's almost like, I suppose, like rolling back a bit

40:26-40:29
from where I've come from creatively and then

40:29-40:32
moved into marketing and brand.

40:32-40:34
I see it playing out.

40:34-40:35
And like, you guys like the value truck curve,

40:35-40:37
all that sort of stuff playing out

40:37-40:40
that you really need to be looking at your brand

40:40-40:44
and your purpose kind of now in terms of where the value is

40:44-40:47
shifting and where you want to be at in sort of like 3, 4,

40:47-40:51
5, however many years when it all plays out.

40:51-40:54
Because that's going to be the big tell of survival

40:54-40:58
and really just like enjoyment of life and what you do,

40:58-41:02
why you wake up in the morning.

41:02-41:05
Because although, like, if you're not,

41:05-41:07
that's where your motivation comes from.

41:07-41:11
And that's where all those little human interactions

41:11-41:13
and all that sort of stuff, all those little things that

41:13-41:16
come from the doorman experience and people wanting

41:16-41:19
to walk through the door in the first place as opposed to,

41:19-41:21
like, why do I even need to walk through?

41:21-41:25
But there's like an aspect here where it's also highly

41:25-41:26
subjective.

41:26-41:27
It really depends-- like, that purpose question really

41:27-41:29
depends on who you're talking to.

41:29-41:30
We're kind of talking in the macro

41:30-41:32
about people who are running a business.

41:32-41:36
If you go back like a few decades

41:36-41:39
and you speak to a typist about whether or not

41:39-41:41
they find a purpose in the work they do,

41:41-41:43
they probably tell you, yeah, this is really meaningful

41:43-41:45
and it's really important because a bunch of people

41:45-41:49
need me to type up these minutes and type up these notes.

41:49-41:51
But for a vast chunk of people, typing

41:51-41:53
was seen to be beneath them.

41:53-41:55
And that's why you had the typist, right?

41:55-41:57
And then you fast forward a couple of decades

41:57-41:58
and everyone's typing because everyone's

41:58-42:00
got a computer in their office.

42:00-42:04
And I think that question is--

42:04-42:06
it'd be super easy for us to sit and pontificate on this

42:06-42:07
for everybody.

42:07-42:10
But it's ultimately a very personal question.

42:10-42:13
It's like, what do you find purpose in the moment

42:13-42:16
is also relevant here to explore?

42:16-42:19
That's not to say that some of the things that we do now,

42:19-42:23
we're not going to do in five years, very, very likely.

42:23-42:24
And that might be a good thing.

42:24-42:26
But this comes back to this question again.

42:26-42:29
It's like, but what did we lose that was valuable?

42:29-42:32
And that's also hard to--

42:32-42:34
I think the cool thing that sits in this space as well

42:34-42:38
is that there's your purpose and values and things

42:38-42:39
that people would have these.

42:39-42:43
And then there's the sort of bullshit corporate overlay

42:43-42:46
of purpose and values that you might convince yourself of.

42:46-42:49
Because you're just doing what you're doing

42:49-42:51
because that's what you do.

42:51-42:52
And it's how you make money.

42:52-42:54
And you need money to live and eat and the rest of it.

42:54-42:58
And it might just be completely orthogonal to what

42:58-43:01
your purpose actually is to that sort of thing.

43:01-43:04
I don't think everybody sits around only really doing things

43:04-43:08
that they're maximally value aligned with.

43:08-43:12
And so you might say, like, it's like, oh, yeah, my typing.

43:12-43:13
It's my purpose.

43:13-43:15
It's my dream.

43:15-43:16
It might not be as well.

43:16-43:19
I've just convinced themselves of this.

43:19-43:21
But I think there's also a big--

43:21-43:24
it's like they're coming out of the matrix part of the thing

43:24-43:24
there, right?

43:24-43:28
If suddenly you're in that headspace

43:28-43:30
and that's what you're convincing yourself of.

43:30-43:32
And then we go, this job no longer exists.

43:32-43:34
And you go, all right, well, I move--

43:34-43:36
no, that one also no longer exists.

43:36-43:37
Like these jobs aren't there anymore.

43:37-43:42
And you're like, oh, shit, now I have to actually figure out

43:42-43:44
what to do with myself.

43:44-43:46
And I've got all this time.

43:46-43:47
And I've got a mortgage.

43:47-43:49
And I've got debts to pay.

43:49-43:50
And I've got children.

43:50-43:52
And then you're like, fuck.

43:52-43:54
That's like a real existential question.

43:54-43:57
There's a real crisis moment there if this hits quicker

43:57-44:01
than some people might expect.

44:01-44:04
I've been thinking about that a bit lately

44:04-44:11
and had a couple of conversations recently where it'll be--

44:11-44:14
I'm almost seeing it more as though every business,

44:14-44:19
every purpose, there's a to-do list on every desk that

44:19-44:21
has stuff on it that just doesn't get done,

44:21-44:23
because they don't have time to get to it.

44:23-44:27
Or like the inertia and all the little grind work

44:27-44:33
just gets in the way and doesn't get to do this stuff.

44:33-44:35
I reckon markets and all that to-do list stuff

44:35-44:39
will start happening if you take away the grind work

44:39-44:42
with AI automations and all that sort of stuff.

44:42-44:43
Maybe.

44:43-44:44
It's weird, though, because it implies

44:44-44:47
that there was something more valuable than that.

44:47-44:51
Because it was not valuable enough that work to just not

44:51-44:51
do it.

44:51-44:55
And the grind work was more valuable than that.

44:55-44:56
It's kind of weird, the idea that I

44:56-44:59
would be able to automate the really valuable grind work,

44:59-45:03
but not the non-grind work that I didn't have time to get to.

45:03-45:04
It's weird that it would move.

45:04-45:07
Could just be the inefficiencies of organizations

45:07-45:10
and the way in how humans interact in organizations

45:10-45:13
and all that sort of stuff.

45:13-45:14
I mean, I think there will be other work.

45:14-45:17
I don't know if it's necessary at the end of the to-do list.

45:17-45:18
I guess.

45:18-45:19
To-do list changes.

45:19-45:24
Unprioritized all this work beyond just grind.

45:24-45:26
It's like, I'm just going to grind for eight hours.

45:26-45:28
Then I'll do the valuable stuff.

45:28-45:30
It's probably not that valuable.

45:30-45:33
Yeah, if you can put it off endlessly, it probably is.

45:33-45:33
That might be a bit.

45:33-45:35
Yeah, that's a tell.

45:35-45:38
But also, this is David Graber's bullshit jobs, right?

45:38-45:40
I think everyone's got--

45:40-45:42
in every role, there's a layer of bullshit jobs.

45:42-45:44
It's a whole tower of bullshit jobs.

45:44-45:47
Yeah, I know.

45:47-45:50
We're sat in an area where there's a lot of bullshit jobs.

45:50-45:52
Yeah, I think I'm going to do this thing.

45:52-45:54
It's more like when you do your strategic planning

45:54-45:56
and all that, and it's like, oh, well, here's

45:56-45:59
all the good ideas we could be doing.

45:59-46:03
We only have the resources to do this one or these two.

46:03-46:05
And then those ones, the ones that--

46:05-46:08
there's that opportunity in that stuff.

46:08-46:12
To argue against my stuff, I'd say certainly in the SMB space,

46:12-46:14
there's just way more of that constraint

46:14-46:16
than that people can't get into those jobs.

46:16-46:19
And they don't have staff lying around to do it.

46:19-46:23
Just-- that's why I find that space more interesting

46:23-46:26
in many ways, because it's the area where you're already

46:26-46:28
resource constrained with people,

46:28-46:30
and you had bigger plans and ideas,

46:30-46:32
and you could never deliver them.

46:32-46:34
And so it would be really interesting to use AI

46:34-46:35
to accelerate this.

46:35-46:38
And you actually get to go and do all this stuff.

46:38-46:40
And then we get really efficient smaller companies.

46:40-46:43
And like that, I think it would be great for the world.

46:43-46:48
Versus like just another big corporate game, 10%

46:48-46:49
better every year.

46:49-46:51
It doesn't really interest me.

46:51-46:54
This is also an aspect of the value trap

46:54-46:58
that I've been thinking about as well, because there is this

46:58-47:03
debate I have with myself about whether or not

47:03-47:07
some companies will actually just go and accelerate hiring.

47:07-47:11
Because especially where there are--

47:11-47:13
there's a heavy engineering dominance in the business,

47:13-47:16
because they can go and--

47:16-47:18
obviously, the first part of the value trap

47:18-47:21
is this kind of automation part of the process

47:21-47:24
where you're dropping your cost curve.

47:24-47:30
They can go and attack entirely new addressable markets

47:30-47:32
with the same number of people, and might just go,

47:32-47:32
do you know what?

47:32-47:35
All of our people armed with all of these agents

47:35-47:37
can actually just go and build a whole heap of new things

47:37-47:40
that this business doesn't provide now.

47:40-47:43
And so the addressable market of a particular

47:43-47:47
business might be x, but now it's 5x,

47:47-47:50
because they've just gone and attacked multiple different markets

47:50-47:51
in one go.

47:51-47:53
And I'm like, oh, how might that play out?

47:53-47:56
The problem is that's not a uniform experience.

47:56-48:01
Not every business is made up of heavy engineering dominance,

48:01-48:04
or people who have the skills to go and work with AI

48:04-48:08
and actually are motivated to go and attack new problems.

48:08-48:11
So the reason why I keep coming back to the value trap

48:11-48:14
is I just think that's more of a uniform explanation for what's

48:14-48:16
more likely to happen.

48:16-48:18
And these kinds of businesses that might actually

48:18-48:21
go and seize that opportunity to increase headcount

48:21-48:21
are probably outlier.

48:21-48:28
As you're saying that, it's more like the limiting--

48:28-48:33
it's identifying new problems, or it's

48:33-48:38
like the innovation component as opposed to--

48:38-48:41
like, oh, the addressable market is here now.

48:41-48:48
The actual-- that is a lag of a new innovation thing happening

48:48-48:49
in that space.

48:49-48:53
Like, if I was out there hiring, I'd

48:53-48:58
be going out to hire as many different kind of thinkers.

48:58-49:04
Like, if we go back to the sort of industrial schooling

49:04-49:12
and all that sort of stuff, the people who are almost outcast

49:12-49:15
and don't fit into that, you can then think different.

49:15-49:20
Like, the dudes who drop out and then go become Richard

49:20-49:23
Branson or all that sort of stuff,

49:23-49:26
they get them on your payroll.

49:26-49:27
The autists.

49:27-49:29
[LAUGHTER]

49:29-49:31
Well, this is what I'm saying.

49:31-49:33
I don't name anyone, but there's obviously

49:33-49:35
someone we know who said the only thing they looked for

49:35-49:36
when they were raising their kids was they just

49:36-49:38
needed to be different.

49:38-49:40
And again, they were very deep in the AI space

49:40-49:42
and could see all this coming.

49:42-49:45
We're just like, it's going to be really valuable to not

49:45-49:51
be just a standard slave from the factory.

49:51-49:54
So if you've got the--

49:54-49:56
like, your kids have brains that just can't be--

49:56-50:02
there's that thing about by year three,

50:02-50:05
that your brain becomes institutionalized.

50:05-50:10
And you go-- and like, a line's with where if you go back

50:10-50:13
to kids and you give them creativity, like, things.

50:13-50:18
And it's like, like, 99% of kids of a certain age

50:18-50:20
are like genius, creative geniuses.

50:20-50:22
But then it almost gets educated out of them.

50:22-50:24
As soon as they get out of them.

50:24-50:24
Yeah.

50:24-50:26
And then they become institutions.

50:26-50:28
Yeah, they're becoming institutionalized.

50:28-50:31
And they're like, hang on, I don't need to think of an answer.

50:31-50:34
I need to think of the answer, that sort of thing.

50:34-50:39
This was like a great TED Talk when TED Talks used to be good

50:39-50:43
by Sir Ken Robinson, who speaks about, like, do

50:43-50:44
schools kill creativity?

50:44-50:46
Do you remember?

50:46-50:48
It was probably one of the best TED Talks

50:48-50:49
that have ever been produced.

50:49-50:52
And it must be like 10, 15 years ago now.

50:52-50:54
I can't remember exactly what that was about.

50:54-50:57
But it was, do schools kill creativity?

50:57-50:58
And that's exactly what you're describing.

50:58-50:59
The answer is yes.

50:59-51:00
Yes, I do.

51:00-51:03
It's been designed to shut up and get in line.

51:03-51:05
Yeah.

51:05-51:06
That's interesting, though.

51:06-51:08
Is that something else that you guys have already done?

51:08-51:10
You're going to become a home to your life.

51:10-51:12
In the last few episodes, I know you said you're a few behind.

51:12-51:14
We've talked a lot about, like, what's

51:14-51:17
the thing that remains at the end of all of this?

51:17-51:20
And we said, like, it is, like, it's the embodiment

51:20-51:21
of the soul.

51:21-51:24
Like, you are the qualia for the system.

51:24-51:26
Like, these things are going to do a lot of the work

51:26-51:27
that you need to do.

51:27-51:31
But what they can't do is live in the world, necessarily

51:31-51:33
and experience things and experience something

51:33-51:37
that's slightly suboptimal from, like, something

51:37-51:40
that you could imagine it could be or slightly better,

51:40-51:41
like, in that way that there's--

51:41-51:43
no matter how life--

51:43-51:44
like, how good life gets for us.

51:44-51:47
And our life is, like, way better than every the ape's life,

51:47-51:48
for instance.

51:48-51:48
Yeah.

51:48-51:52
Like, there's always, like, it's not better.

51:52-51:55
Like, there's always something else that you long for.

51:55-51:56
There's almost that human longing,

51:56-52:00
like, it's like, never quite happy that pushes you forwards.

52:00-52:03
And then, like, it's the, like, OK, capture that.

52:03-52:05
And then your job is to capture that and assemble,

52:05-52:09
like, an army around you to make that problem go away

52:09-52:11
as quickly as possible.

52:11-52:12
And then that's how you'll get paid.

52:12-52:14
And so it's almost like--

52:14-52:16
it's like an entrepreneur's dream,

52:16-52:19
is that you can just go about figuring out problems,

52:19-52:21
experimenting with solutions, and getting the matter

52:21-52:23
into market very quickly.

52:23-52:26
And then you move on, and you do it again, and again, and again.

52:26-52:26
Yeah.

52:26-52:29
And your job is to kind of, like, experience the world

52:29-52:32
and then, you know, shave off the reflages.

52:32-52:33
Yeah.

52:33-52:36
And that's, like, where--

52:36-52:36
how's that?

52:36-52:39
Like, that's the exciting part to me.

52:39-52:43
Going back to, like, 100 years of progress in, like, a decade,

52:43-52:44
you play that out.

52:44-52:46
And all of a sudden, there's, cool,

52:46-52:47
here's a wicked idea.

52:47-52:50
And I can, like, pump it out and get it happening,

52:50-52:51
and pump it out and get it happening.

52:51-52:54
Almost like that, raising the ceiling of genius

52:54-52:55
and scaling genius.

52:55-52:59
It's like, then, all of a sudden, the better ideas

52:59-53:02
onto the surface doesn't matter if you're

53:02-53:04
a multi-billion dollar, like, R&D, when you've got R&D

53:04-53:06
and innovation, all this sort of stuff,

53:06-53:12
and you can take it to market versus your, like, college kid

53:12-53:16
in a garage doing something, like, that sort of stuff.

53:16-53:18
And we just drop those barriers to entry as well, right?

53:18-53:20
So it's not even like--

53:20-53:22
at the moment, there's definitely, like,

53:22-53:24
a centralization of a lot of that innovation

53:24-53:26
experimentation into the States, and then, particularly,

53:26-53:28
into, like, San Francisco.

53:28-53:31
That's probably moved a bit in the last few years.

53:31-53:34
But, like, if you make it so that everybody

53:34-53:36
has all that intelligence and all that knowledge

53:36-53:39
from anywhere on Earth, then you end up just, in my view,

53:39-53:42
at least, just niching down into, like, all right, well,

53:42-53:45
there's, like, yes, they may have a solution for that problem

53:45-53:48
over there, but they don't experience things quite

53:48-53:50
like how I do in Perth, which has a different set of--

53:50-53:52
like, you know, you were talking about earlier,

53:52-53:53
I live in a different way.

53:53-53:54
I have a different network.

53:54-53:56
I have different norms in the society that I'm in.

53:56-53:59
So I would solve it slightly differently.

53:59-54:02
And the solution for me will work much better in this context.

54:02-54:05
So I think you start to then see the same sort of solutions

54:05-54:06
scrubbing up all over the place, but it's

54:06-54:09
slightly different context that makes all the difference

54:09-54:13
for how it plays out inside, like, that particular society.

54:13-54:15
So I get excited about this.

54:15-54:17
So to me, this is, like, how you decentralize a lot of the power

54:17-54:19
in the world.

54:19-54:21
Everybody starts to take more power on themselves

54:21-54:24
and have more of a locality in a community.

54:24-54:27
So the norms of that community start getting forced around

54:27-54:27
there.

54:27-54:30
And then that just undermines everything else on top of it.

54:30-54:32
It's the norms that matter.

54:32-54:35
So if we can capture the norms, I'm good.

54:35-54:36
It's all done.

54:36-54:40
That's sort of like one of the things I see playing out

54:40-54:43
is when going back to the typist and finding purpose

54:43-54:45
and all these sort of stuff.

54:45-54:48
And it kind of plays to the stuff you're doing,

54:48-54:49
like building things.

54:49-54:51
And then the bit that the AR can't do it, pings off.

54:51-54:56
And there's-- there'll be a shift to almost like a--

54:56-54:59
or a variation of, like, that gig economy where everyone is

54:59-55:02
almost like their own solo operator.

55:02-55:06
And they can do the thing that they really want to do.

55:06-55:08
It's like in a world of abundance,

55:08-55:11
where you can get everything you want.

55:11-55:14
The decision is to figure out what you want.

55:14-55:15
We have to.

55:15-55:19
And that becomes the thing, which is the purpose question.

55:19-55:23
But so in this gig economy sort of thing,

55:23-55:24
everything becomes localized.

55:24-55:26
And you don't--

55:26-55:28
like, what is having a business?

55:28-55:32
And what is-- like, you don't need an office space,

55:32-55:34
because you don't need to have all these people coming in,

55:34-55:38
because you're getting all your work done decentralized.

55:38-55:42
But then the office space is also for the human community

55:42-55:43
aspect.

55:43-55:45
And that's where the--

55:45-55:48
like, I see like little community hubs or co-working

55:48-55:51
things like this, where there's hopefully

55:51-55:53
like daycares and craches and all this sort of stuff

55:53-55:56
to take care of all your needs or that sort of stuff.

55:56-55:57
Like, that will just become--

55:57-55:59
Well, the kids just mingle.

55:59-56:00
Well, yeah, there's--

56:00-56:02
You just have like a bunch of toys in the corner.

56:02-56:03
Ooh.

56:03-56:05
There's-- is that Simon?

56:05-56:05
Scooper Simon?

56:05-56:09
There's like the actual--

56:09-56:12
like, kids being actually grouped into ages.

56:12-56:12
Yeah, it's fun.

56:12-56:13
And pushed through schools.

56:13-56:15
It's actually like a different thing,

56:15-56:17
where like different educational things,

56:17-56:20
you have like the teenagers mixed in,

56:20-56:22
and they help educate the other kids and all this sort of--

56:22-56:24
like, all that sort of stuff.

56:24-56:28
Like, that's how I say like all of this evolving.

56:28-56:30
It'll be interesting, because then you go like,

56:30-56:31
well, where are the resources?

56:31-56:33
And it's like, well, what do schools become?

56:33-56:34
Is that the evolution of a school?

56:34-56:36
You just like-- it's not a construction company

56:36-56:38
that redesigned schools and repurposes them

56:38-56:41
into these community hubs.

56:41-56:43
Something like that.

56:43-56:45
I feel like the institution of school,

56:45-56:48
like, we're seeing all that say bad things about schools.

56:48-56:51
But I think it's just become like a bit of a--

56:51-56:53
What's it to the principal's office?

56:53-56:55
Well, it is a bit of like a prison for kids

56:55-56:58
that is there to mold them into good factory workers.

56:58-57:01
And that's where it all came from back in the day,

57:01-57:02
from that impression system of like, all right,

57:02-57:04
we need a really compliant population,

57:04-57:06
so that we can send them to war to die.

57:06-57:09
And it's like, all right, that's--

57:09-57:11
it's not what I most want for my children,

57:11-57:13
is to go to war and die.

57:13-57:15
Like, that sounds pretty horrific.

57:15-57:17
So maybe there is a better way of doing it.

57:17-57:22
But it's become so ingrained in society, like, all right,

57:22-57:24
government is going to pave your school,

57:24-57:25
and you have to go to school.

57:25-57:28
And if kids didn't go to school, like,

57:28-57:31
it's such an imposition that they're always around the house

57:31-57:33
and say, well, how am I going to think of anything

57:33-57:34
or interact with anybody?

57:34-57:37
I think it has to only come from like,

57:37-57:40
it's just a different norm in society of like--

57:40-57:43
you go to like, a world school in place or something like that.

57:43-57:45
And they're just-- everyone's there with their kids,

57:45-57:45
and they're working.

57:45-57:47
And sometimes the kids go with that person.

57:47-57:49
Sometimes the kids go with that person.

57:49-57:51
It's just a slightly different way of living.

57:51-57:54
But it doesn't fit in this--

57:54-57:57
It's almost like traveling into a scale.

57:57-57:59
Yeah, I think that tribes are a great example of--

57:59-58:03
I think we would all feel very good in a tribe,

58:03-58:04
if you know what I mean.

58:04-58:05
I mean, there's something about that.

58:05-58:06
There's a very natural way to live.

58:06-58:07
Yeah.

58:08-58:10
Um.

58:10-58:14
We did a whole episode on this, by the way.

58:14-58:16
On education.

58:16-58:17
So if anyone's listening and wants to--

58:17-58:18
[LAUGHTER]

58:18-58:19
I can't.

58:19-58:21
The card just popped up in the left corner there, maybe.

58:21-58:24
We'll see if we do any editing.

58:24-58:27
Because I just think this is such a fascinating--

58:27-58:28
I can't remember what we're talking about.

58:28-58:29
Learning.

58:29-58:31
It's something about the future of learning.

58:31-58:32
It's something--

58:32-58:34
It's permissionless, maybe.

58:34-58:39
Yeah, because if you think about it,

58:39-58:42
this has been a problem for schooling since the internet.

58:42-58:44
I mean, as the cost of information

58:44-58:46
keeps trending towards zero, what's

58:46-58:48
the value of education?

58:48-58:50
Because it's essentially just--

58:50-58:54
you've got a medium to provide information from a syllabus

58:54-58:56
to as many people as possible.

58:56-58:59
And it's generally like one facilitator or teacher

58:59-59:01
to 30 kids in a classroom.

59:01-59:05
And then that's kind of the scale that we've found works.

59:05-59:09
When we know that the ideal state is as close to one

59:09-59:10
to one as possible.

59:10-59:12
So that's where tuition is just really valuable,

59:12-59:14
especially if you-- to your point,

59:14-59:16
earlier you can start to tailor it to learning styles

59:16-59:17
and things like that.

59:17-59:18
But I mean--

59:18-59:21
I might even remove the one to one.

59:21-59:22
It's like it should be fluid.

59:22-59:23
Yeah.

59:23-59:26
Because one to one loses social interactions

59:26-59:27
and all that sort of stuff.

59:27-59:30
And you need to have split attention

59:30-59:34
to going from two kids to three kids.

59:34-59:36
And all of a sudden there's four ways

59:36-59:37
one that doesn't have attention.

59:37-59:38
Yeah.

59:38-59:40
It completely changes dynamics.

59:40-59:41
But you've got to learn all that sort of stuff.

59:41-59:46
So it's almost-- you need a fluid ratio across things

59:46-59:48
and just more of a fluid thing anyway.

59:48-59:51
I think the other thing about individualized learning

59:51-59:55
is it taps into people's natural feedback loops.

59:55-59:58
Because the thing that gives you--

59:58-01:00:00
it shortens the feedback loop to learning,

01:00:00-01:00:02
which is just progress.

01:00:02-01:00:04
The thing that shortens that is the thing

01:00:04-01:00:08
that ensures that that learning becomes addictive.

01:00:08-01:00:10
I think that's the thing that I've found with AI.

01:00:10-01:00:14
I've actually found myself really enjoying just diving

01:00:14-01:00:17
into the process of learning a bunch of stuff with AI.

01:00:17-01:00:21
And I'm not using AI to shortcut anything.

01:00:21-01:00:25
I'm just using it as a tool to help me learn in a way that

01:00:25-01:00:26
suits me.

01:00:26-01:00:29
And then I've just found myself going down rabbit holes

01:00:29-01:00:32
constantly because it's just so addictive.

01:00:32-01:00:35
Because the feedback loop is close to instant for me.

01:00:35-01:00:37
Whereas I didn't--

01:00:37-01:00:40
I did well in school, but I didn't necessarily

01:00:40-01:00:41
enjoy school.

01:00:41-01:00:43
Like, I think it was just something that it was like,

01:00:43-01:00:44
I have to do this.

01:00:44-01:00:45
Everyone has to do this.

01:00:45-01:00:46
And I've really thought too much about it

01:00:46-01:00:47
until I left school.

01:00:47-01:00:49
It was almost like the game you were playing.

01:00:49-01:00:50
You didn't know why you were playing it.

01:00:50-01:00:51
You knew you were playing it.

01:00:51-01:00:54
But if I'm going to play it, I may as well do it well.

01:00:54-01:00:56
Because we're all naturally competitive.

01:00:56-01:00:57
I think that was the thing about art school as well.

01:00:57-01:00:59
There was a lot of healthy competition that

01:00:59-01:01:01
was just baked into us.

01:01:01-01:01:04
That was pretty about like late 20s before I woke up.

01:01:04-01:01:06
I was like, why am I doing any of this shit?

01:01:06-01:01:08
Like, what's going on?

01:01:08-01:01:11
I mean, for me, I think it finally

01:01:11-01:01:14
tits like maybe second year of law school,

01:01:14-01:01:16
where I was just like, this is just

01:01:16-01:01:18
super tracked everyone's hyper competitive.

01:01:18-01:01:22
And I was just like, what the fuck am I doing?

01:01:22-01:01:23
This just doesn't--

01:01:23-01:01:26
like that to me was like the fastest the hamster wheel

01:01:26-01:01:28
was ever going.

01:01:28-01:01:30
Because you were just like constantly--

01:01:30-01:01:33
and I was sat there in lectures.

01:01:33-01:01:36
Everyone is absolutely loving everything they're doing.

01:01:36-01:01:38
And I'm sat there like thinking about a business

01:01:38-01:01:40
that I want to start.

01:01:40-01:01:42
And then like a year later--

01:01:42-01:01:43
I didn't know they liked "Renell's Love,"

01:01:43-01:01:44
but they were doing that.

01:01:44-01:01:45
What if they were oldest?

01:01:45-01:01:46
I mean, I think it was--

01:01:46-01:01:49
I'm sure there were other people who were like me.

01:01:49-01:01:50
There absolutely were.

01:01:50-01:01:53
I said, my purpose is to read documents super closely.

01:01:53-01:01:56
But in the main, in the main, a lot of the guys

01:01:56-01:01:58
that I went to, you knew it, went to law school with.

01:01:58-01:01:58
Absolutely loved it.

01:01:58-01:02:01
Like it just does naturally.

01:02:01-01:02:03
You tend to gravitate toward--

01:02:03-01:02:05
It probably takes a lot of boxes for people as well, right?

01:02:05-01:02:06
It's a good place in society.

01:02:06-01:02:08
People respect you for what you're doing.

01:02:08-01:02:09
Yeah.

01:02:09-01:02:10
You're seen in a certain way.

01:02:10-01:02:13
You see how that plays out in the next decade.

01:02:13-01:02:14
Even shorter.

01:02:14-01:02:17
I think it just gets destroyed, but it's fine.

01:02:17-01:02:21
But like the professional, it'll be a shake up.

01:02:21-01:02:26
But you're going to grow up and become a doctor.

01:02:26-01:02:28
You're going to grow up and become a doctor.

01:02:28-01:02:31
You're going to be an experienced when you're older.

01:02:31-01:02:32
Like there's this--

01:02:32-01:02:37
I did a stint at a premium/luxury,

01:02:37-01:02:39
but luxury is a perp brand.

01:02:39-01:02:44
But there's a lot of money in perp.

01:02:44-01:02:47
I'm sure the luxury brands can do all right.

01:02:47-01:02:52
I know the luxury brands come into perp from other places.

01:02:52-01:02:53
OK.

01:02:53-01:02:55
We don't have to go down that path.

01:02:55-01:02:56
You bet the thing--

01:02:56-01:02:59
Oh, like there's a couple of dudes who--

01:02:59-01:03:01
I forget what companies they're at.

01:03:01-01:03:04
But like a regional sort of luxury brand dudes wrote

01:03:04-01:03:06
this luxury strategy book.

01:03:06-01:03:11
And it's all about how there was like kings and queens

01:03:11-01:03:13
and lords and the noblemen and all this

01:03:13-01:03:15
sort of like social hierarchy.

01:03:15-01:03:18
And then when that disappeared, the luxury brands

01:03:18-01:03:20
almost took that sort of social stratification

01:03:20-01:03:23
and laid it in.

01:03:23-01:03:26
And you go like, what does that look like from it?

01:03:26-01:03:29
Because that is a tell on human interaction.

01:03:29-01:03:32
It's just thinking about humans like societies.

01:03:32-01:03:33
Yeah.

01:03:33-01:03:38
So it's kind of like, what's the next version of that?

01:03:38-01:03:39
Yeah.

01:03:39-01:03:42
Which is sort of like a new--

01:03:42-01:03:45
Because you have like the rich and the nouveau rich, right?

01:03:45-01:03:49
Which is like way more pronounced maybe in the UK than it is here.

01:03:49-01:03:52
But I wonder if there's like a nouveau, nouveau rich sort

01:03:52-01:03:54
of coming where there'll be the people that, you know,

01:03:54-01:03:58
I got rich by being a doctor.

01:03:58-01:03:58
Yeah.

01:03:58-01:03:59
Look down.

01:03:59-01:04:01
You know, I feel like we kind of touched on this a little bit

01:04:01-01:04:04
with our first guest, Joel Pemberton,

01:04:04-01:04:05
a couple of episodes ago.

01:04:05-01:04:08
But like-- because I think this is like a really interesting

01:04:08-01:04:08
point.

01:04:08-01:04:10
It's like, status doesn't disappear.

01:04:10-01:04:11
Desire doesn't disappear.

01:04:11-01:04:14
We kind of like hovered on these things in this chat.

01:04:14-01:04:19
And the point that I was asking Joel about was like,

01:04:19-01:04:21
it feels to me in this world of abundance

01:04:21-01:04:24
when you still have those two states that are pretty dominant.

01:04:24-01:04:28
Everyone's still chasing status and they still have desires.

01:04:28-01:04:32
It feels to me that brand becomes the way to like tap into that.

01:04:32-01:04:36
I feel it becomes like even more valuable in a world of abundance

01:04:36-01:04:38
in order to create this artificial scarcity.

01:04:38-01:04:41
Because that's essentially what the brand has to do in that state.

01:04:41-01:04:44
And I'm kind of curious, like just to like link it back to your--

01:04:44-01:04:46
the expertise and the way you're thinking about all this stuff.

01:04:46-01:04:47
Like, how do you see brand?

01:04:47-01:04:48
I'd agree with that.

01:04:48-01:04:52
I haven't listened to the episode yet.

01:04:52-01:04:57
But I know like the G-Sports guys sponsor all their life,

01:04:57-01:04:59
hooked in with their own big bass and all that.

01:04:59-01:05:01
So they're like thinking about brand right.

01:05:01-01:05:04
Like a lot of people don't even think about brand right

01:05:04-01:05:07
in the first place.

01:05:07-01:05:10
But I also-- so the way I see it playing out

01:05:10-01:05:12
and the way I'm sort of structuring my stuff

01:05:12-01:05:16
is like the AI automation and that sort of piece,

01:05:16-01:05:18
the organizational, like the human strategy piece,

01:05:18-01:05:21
but then the brand strategy piece.

01:05:21-01:05:24
And that is actually the--

01:05:24-01:05:32
because brand is a human psychology like thing.

01:05:32-01:05:35
And it's your go and you can have a rack of clothes

01:05:35-01:05:38
or you can have a fridge full of water bottles

01:05:38-01:05:39
or you could have a whatever.

01:05:39-01:05:42
And you'll be attracted to the one that like appeals

01:05:42-01:05:44
to you more or that you relate to more

01:05:44-01:05:46
or that you-- that sort of thing.

01:05:46-01:05:54
So if everything is abundant and for the things that--

01:05:54-01:05:56
I sort of saying this goes back to what--

01:05:56-01:05:58
it's like what do you do as humans?

01:05:58-01:06:00
What do you want to do?

01:06:00-01:06:01
Where do you spend your time?

01:06:01-01:06:02
And this will be different for everyone right.

01:06:02-01:06:04
It's like some people won't care about water

01:06:04-01:06:06
and they'll just go buy--

01:06:06-01:06:09
well, if you need to buy it, they'll just go get water.

01:06:09-01:06:12
But then some people might go, well, I actually

01:06:12-01:06:15
am in a period of my life where I want to look like this

01:06:15-01:06:17
or appeal like this or like.

01:06:17-01:06:19
Want to be this sort of status.

01:06:19-01:06:23
So I'm going to go get that one to then be a part of that sort

01:06:23-01:06:24
of click or that little thing.

01:06:24-01:06:31
And that's where the brand stuff will come into play.

01:06:31-01:06:33
For small businesses or that sort of stuff,

01:06:33-01:06:35
that's also what--

01:06:35-01:06:41
that's quite funny of working through some stuff.

01:06:41-01:06:46
And did this problem basically had

01:06:46-01:06:48
to resolve something in this way.

01:06:48-01:06:52
I'm structuring things, but it was like a brand thing.

01:06:52-01:06:57
And I used Claude Prometedit to play

01:06:57-01:07:03
the role of Mark Ritzen, Byron Sharp, John James, Tom

01:07:03-01:07:05
Roach, someone like just a whole heap of brand

01:07:05-01:07:07
creative dudes.

01:07:07-01:07:10
And I went, here's my problem.

01:07:10-01:07:16
I want you guys to discuss it and come back and let's

01:07:16-01:07:17
have a brainstorm.

01:07:17-01:07:21
And that was it, right?

01:07:21-01:07:25
And the actual generation came back almost like a script

01:07:25-01:07:26
sort of thing.

01:07:26-01:07:32
And it was like these creative dudes or whatever brand

01:07:32-01:07:36
specialists sit down in a pub and they have like a--

01:07:36-01:07:38
And I'm like, whoa, I've never--

01:07:38-01:07:41
Anything about how LLMs are all probably sticking or that.

01:07:41-01:07:44
And from that, the most probable thing

01:07:44-01:07:48
was sitting them in a pub to do this.

01:07:48-01:07:50
And then they have this back and forth

01:07:50-01:07:51
conversation is quite--

01:07:51-01:07:53
You can do this.

01:07:53-01:07:55
Requires a bit of infrastructure to get put in.

01:07:55-01:07:59
Maybe I'll code this, actually, which is interesting.

01:07:59-01:08:02
But it's to have people talk backwards and forwards

01:08:02-01:08:03
like on LLMs.

01:08:03-01:08:05
So you can just have agents set up.

01:08:05-01:08:07
So you're going to talk about this,

01:08:07-01:08:08
and you're going to talk about this.

01:08:08-01:08:10
Here's you, here's you, here's you.

01:08:10-01:08:13
Give them all different insights, all different backgrounds,

01:08:13-01:08:14
all different system prompts.

01:08:14-01:08:16
And then you have them play out in a dialogue,

01:08:16-01:08:18
like the solution to something.

01:08:18-01:08:19
And you can get--

01:08:19-01:08:23
You might run it for 20 iterations or something like

01:08:23-01:08:23
this.

01:08:23-01:08:26
And then people come back to it and just go, oh shit,

01:08:26-01:08:28
they actually really got somewhere because you see the

01:08:28-01:08:30
evolution of the idea over time.

01:08:30-01:08:32
It's like someone will say something and the next person

01:08:32-01:08:34
will come to you and go, oh, I really like that point, that

01:08:34-01:08:34
point, that point.

01:08:34-01:08:36
That point feels a bit weak to me.

01:08:36-01:08:39
And I had a look over here online, and here's some stuff

01:08:39-01:08:39
we could look into.

01:08:39-01:08:41
And he said, oh, yeah, that's a great point.

01:08:41-01:08:44
Or he then defends the point.

01:08:44-01:08:45
No, I was thinking about it like this.

01:08:45-01:08:48
And they start to work out all of those little kinks in

01:08:48-01:08:52
dialogue in a way that is really interesting to see.

01:08:52-01:08:56
So there's a guy, Pablo, who's a sovereign engineering

01:08:56-01:08:57
course over in Madeira.

01:08:57-01:08:59
It's one of the things he's built for his software

01:08:59-01:09:03
engineering tools is that it starts in dialogue with itself.

01:09:03-01:09:06
He will set an intent for what they need to design.

01:09:06-01:09:10
And then they set up all these different agents of all the

01:09:10-01:09:14
beliefs and values of the particular systems that he's

01:09:14-01:09:15
building around Bitcoin and Nostra.

01:09:15-01:09:19
And then they talk about the idea till they get to a design.

01:09:19-01:09:19
That's just like that.

01:09:19-01:09:20
That's OK.

01:09:20-01:09:21
That's awesome.

01:09:21-01:09:26
There's a platform--

01:09:26-01:09:30
service platform tool, Mind Hive, which is one that I'm

01:09:30-01:09:34
sort of looking at speaking to.

01:09:34-01:09:39
But it's like a consulting tool, AI tool.

01:09:39-01:09:40
And it does that.

01:09:40-01:09:44
So you can create online workshops, which is

01:09:44-01:09:46
essentially just a chat.

01:09:46-01:09:49
But then have invite whoever you want to this thing.

01:09:49-01:09:52
But then also create AI agents.

01:09:52-01:09:54
And AI agents could be any sort of--

01:09:54-01:09:57
like as long as there's enough information out there that

01:09:57-01:09:59
they've been able to learn from, you can wrap it in.

01:09:59-01:10:04
Wrap it in a historical person.

01:10:04-01:10:04
So you could--

01:10:04-01:10:05
and then all of the--

01:10:05-01:10:08
like whatever you want to discuss has those people

01:10:08-01:10:08
talking.

01:10:08-01:10:12
And then they all just chime in and throw their input.

01:10:12-01:10:16
Which that's pretty cool.

01:10:16-01:10:19
They're really good at mimicking and role-playing.

01:10:19-01:10:22
I do this in software a lot.

01:10:22-01:10:26
When I first start a new feature, with Rooko, I go into

01:10:26-01:10:27
the architect role.

01:10:27-01:10:29
And I say, look, here's what I'm trying to achieve.

01:10:29-01:10:32
Now we're going to discuss this for a while backwards and

01:10:32-01:10:34
forwards, so we can come up with a plan.

01:10:34-01:10:35
And it will come up.

01:10:35-01:10:36
OK, I see where you're trying to go.

01:10:36-01:10:38
But here's an area where you haven't considered what's the

01:10:38-01:10:39
answer here.

01:10:39-01:10:40
I answer backwards and forwards.

01:10:40-01:10:41
He proposes a plan.

01:10:41-01:10:42
I say, no, no, no.

01:10:42-01:10:42
We're not done.

01:10:42-01:10:44
Like we're going to come on.

01:10:44-01:10:46
And then eventually we come up with a plan.

01:10:46-01:10:50
But we do it in a way that, for me to one-shot prompt that,

01:10:50-01:10:51
it wouldn't have worked.

01:10:51-01:10:52
It's not possible.

01:10:52-01:10:55
But in it, then inquiring after me over and over again, and

01:10:55-01:10:56
going backwards and forwards.

01:10:56-01:10:58
I remember all the stuff I should have remembered at the

01:10:58-01:10:59
start.

01:10:59-01:11:01
It then gets more and more context as we go.

01:11:01-01:11:04
Then it gets summarized in the plan before we execute it.

01:11:04-01:11:07
And it's such a great way of just running out the edges of

01:11:07-01:11:08
that sort of stuff.

01:11:08-01:11:10
I do this with multiple models as well.

01:11:10-01:11:12
Like I'll have--

01:11:12-01:11:14
you've got the agent, obviously, in cursor.

01:11:14-01:11:15
But I'll go in.

01:11:15-01:11:17
Like that is just executing tasks.

01:11:17-01:11:20
So that is way down the process.

01:11:20-01:11:25
But I'll often have Claude as the engineering lead that's

01:11:25-01:11:27
trying to build out the PRD.

01:11:27-01:11:30
But then I'll take what he's saying, and I'll give it to--

01:11:30-01:11:32
I mean, I've used Grok, I've used ChatGbt as the kind of

01:11:32-01:11:35
late technical co-founder who's trying to think about it on a

01:11:35-01:11:36
macro level.

01:11:36-01:11:39
And it's like you build out a profile, like a role spec for

01:11:39-01:11:41
the model ahead of time.

01:11:41-01:11:43
And then I'm just saying manually, like copying and

01:11:43-01:11:46
pasting bits of chat between them.

01:11:46-01:11:48
And then you do dial--

01:11:48-01:11:50
and I'm obviously steering as well.

01:11:50-01:11:52
So I'm trying to shape this in the way I think it needs to

01:11:52-01:11:52
go.

01:11:52-01:11:55
But yeah, you do get some pretty interesting

01:11:55-01:11:57
outputs.

01:11:57-01:11:59
That needs an interface.

01:11:59-01:12:00
And that's what's been--

01:12:00-01:12:07
that's something that's been bugging me is having what--

01:12:07-01:12:10
like it should be a new OS.

01:12:10-01:12:13
So there should be like AROS or whatever.

01:12:13-01:12:17
And it is that you can basically just sign up or

01:12:17-01:12:19
whatever, call in whatever you want.

01:12:19-01:12:23
And then just go, OK, here's the prompt for this one.

01:12:23-01:12:25
Here's this, here's this, here's this.

01:12:25-01:12:30
Now I'm opening up docs or whatever.

01:12:30-01:12:31
Like--

01:12:31-01:12:33
and hang on.

01:12:33-01:12:34
Oh, you--

01:12:34-01:12:38
I think this is all interactive with the MCP interfaces

01:12:38-01:12:40
and various different things.

01:12:40-01:12:41
It's all coming.

01:12:41-01:12:43
Like you see the difference of like--

01:12:43-01:12:46
the reason so many people use cursor, I think, is because it

01:12:46-01:12:48
is a program on your machine.

01:12:48-01:12:53
And therefore, because it's a software ID particularly, it

01:12:53-01:12:56
has direct access through the terminal to anything your

01:12:56-01:12:57
machine can do.

01:12:57-01:12:57
Exactly.

01:12:57-01:13:00
Like in a command line system without having to like

01:13:00-01:13:02
navigate with mouse and all the rest of it.

01:13:02-01:13:05
Which, you know, like people that work with computers like

01:13:05-01:13:08
that usually understand like how powerful that is and how

01:13:08-01:13:09
much stuff you can do.

01:13:09-01:13:11
But as soon as it gives the access to anything on your

01:13:11-01:13:14
computer from the file system, and it can write and read

01:13:14-01:13:17
whatever it wants, like it basically becomes almost like

01:13:17-01:13:17
a form.

01:13:17-01:13:18
Yeah.

01:13:18-01:13:21
I mean, I saw an example with like a zero MCP.

01:13:21-01:13:22
So it would--

01:13:22-01:13:24
like you've obviously got all of your accounting, your books

01:13:24-01:13:27
in zero, and then you're just using cursor to just generate

01:13:27-01:13:31
quick reports and generating quotes to send out to customers

01:13:31-01:13:31
and invoices.

01:13:31-01:13:34
But it's just like one shot instead of like having to go

01:13:34-01:13:35
through a bunch of--

01:13:35-01:13:39
like a bunch of steps in like a clunky--

01:13:39-01:13:40
I find it clunky.

01:13:40-01:13:41
A lot of people love it.

01:13:41-01:13:42
Zero.

01:13:42-01:13:45
You can just like literally quick prompt output done.

01:13:45-01:13:46
And it's on a link somewhere.

01:13:46-01:13:48
And you can just send someone a link and it's done.

01:13:48-01:13:51
Well, because it can then get that information back and then

01:13:51-01:13:53
run it through whatever computer program it then wants to

01:13:53-01:13:56
write to execute that thing.

01:13:56-01:13:58
You would want a code execution environment and whatever

01:13:58-01:14:00
agent the system is that you've got in the future.

01:14:00-01:14:02
It seemed really counterintuitive when I first

01:14:02-01:14:02
heard this.

01:14:02-01:14:05
And then I started looking at examples of how people were

01:14:05-01:14:05
using it.

01:14:05-01:14:09
And I was just like, oh, like cursor is just the interface

01:14:09-01:14:12
for the entire business now, which is crazy and also annoying

01:14:12-01:14:13
because of--

01:14:13-01:14:13
Or root code.

01:14:13-01:14:14
Or root code.

01:14:14-01:14:15
Yeah.

01:14:15-01:14:19
I mean, maybe more so root code after the like building

01:14:19-01:14:21
disputes on last week.

01:14:21-01:14:22
What was that one?

01:14:22-01:14:24
I used root code.

01:14:24-01:14:27
So I went from cursor to client, which is like an open

01:14:27-01:14:31
source plug-in version that sits inside Visual Studio.

01:14:31-01:14:34
And then client itself was then forked to something called

01:14:34-01:14:35
root code.

01:14:35-01:14:36
And it has like a--

01:14:36-01:14:37
The one different--

01:14:37-01:14:38
The one different O.

01:14:38-01:14:39
R-O-O, like room.

01:14:39-01:14:40
Kangaroo, like.

01:14:40-01:14:41
That's what you're asking.

01:14:41-01:14:42
Come on.

01:14:42-01:14:44
What are you doing?

01:14:44-01:14:49
What it does is it gives you just more direct roles.

01:14:49-01:14:52
So you have asking roles, but then you have an architect,

01:14:52-01:14:56
an orchestrator, a coder, a debugger, and so on.

01:14:56-01:14:59
And particularly the way that it does the orchestration of

01:14:59-01:14:59
the tasks.

01:14:59-01:15:02
So you would plan out at a high level everything with the

01:15:02-01:15:03
architect.

01:15:03-01:15:06
And then instead of going into code and slowly grinding

01:15:06-01:15:07
through it, it goes into--

01:15:07-01:15:10
I push it into this orchestrator mode, which will then split

01:15:10-01:15:14
it out into 40 subtasks that are each nicely constrained.

01:15:14-01:15:15
Just what it needs.

01:15:15-01:15:16
And then it executes those.

01:15:16-01:15:19
And the big difference is that you don't get a massive

01:15:19-01:15:20
blowout in the context.

01:15:20-01:15:23
When I used to do that in client, I would just blow

01:15:23-01:15:25
$10 on creating something.

01:15:25-01:15:27
And then not knowing whether it was good or not.

01:15:27-01:15:31
And it will come out as like $0.40 in root code.

01:15:31-01:15:34
Because it only took that subtext.

01:15:34-01:15:37
And if that subtext then goes off on a loop and gets stuck,

01:15:37-01:15:40
you can pull it out of it and go back to the original plan.

01:15:40-01:15:41
It's just--

01:15:41-01:15:42
It works really well.

01:15:42-01:15:45
I think the other thing in cursor now is that you can

01:15:45-01:15:45
actually--

01:15:45-01:15:49
when you're typing in a new prompt, you can select the

01:15:49-01:15:50
actual file that you just want to review.

01:15:50-01:15:53
Like you can individualize it down.

01:15:53-01:15:55
Which when we first started using it, you couldn't.

01:15:55-01:15:58
It was just going and reviewing the entire code base

01:15:58-01:16:01
every time you put a prompt through, which seemed really

01:16:01-01:16:01
expensive.

01:16:01-01:16:04
And obviously, it doesn't scale particularly well as the

01:16:04-01:16:04
code base grows.

01:16:04-01:16:07
It's just becomes more and more of a nightmare.

01:16:07-01:16:09
I want to get back to actually trying out the

01:16:09-01:16:11
latest stack graph stuff now.

01:16:11-01:16:15
I think that's probably an estate where I could just

01:16:15-01:16:18
run that, have the whole thing locally, index all the code

01:16:18-01:16:21
base, and then pull really good context from the graph and

01:16:21-01:16:23
avoid even more stuff.

01:16:23-01:16:26
I love how we keep teasing graphs every week.

01:16:26-01:16:29
Like every episode we do, we just tease it a little bit

01:16:29-01:16:31
more, but we don't actually talk too much about graphs.

01:16:31-01:16:33
I was thinking just before we start, I was like, I'm going

01:16:33-01:16:38
to build an MCP server that records every prompt that I

01:16:38-01:16:41
make from any of the different applications that I'm

01:16:41-01:16:42
using.

01:16:42-01:16:45
It'll just send this prompt to the MCP server.

01:16:45-01:16:48
And on the back end, I'm going to analyze any new

01:16:48-01:16:49
entry into the graph.

01:16:49-01:16:51
Like I'll analyze it and then pull out what was good or

01:16:51-01:16:53
bad about those prompts.

01:16:53-01:16:57
And then what's the better version of this prompt?

01:16:57-01:17:01
What I want to try and build is a self-reinforcing system so

01:17:01-01:17:04
that every time I start to prompt in the future, I can

01:17:04-01:17:07
use my own MCP server of every prompt I've ever written in

01:17:07-01:17:10
the past and the better version of all those prompts to

01:17:10-01:17:12
then suggest a better version back to me.

01:17:12-01:17:16
Because I was just thinking, I'm so sick of writing the

01:17:16-01:17:19
same shit in this box over and over again.

01:17:19-01:17:20
This is prompting.

01:17:20-01:17:21
What if I could just write--

01:17:21-01:17:22
you know, in the same way we did.

01:17:22-01:17:25
It's like I did with StackWork with the stubbing of the

01:17:25-01:17:26
tickets.

01:17:26-01:17:28
As you got to a point, you wrote enough of these

01:17:28-01:17:31
banses that instead of writing the bans, you just stub

01:17:31-01:17:33
out the bits that are different for that bans, and then it

01:17:33-01:17:36
filled in all the rest from the graph based on all the

01:17:36-01:17:37
examples.

01:17:37-01:17:39
And I was like, I could do this with prompts.

01:17:39-01:17:41
I could just be like, review this, do that, do that, build

01:17:41-01:17:46
one of these, and not have to do half of the prompting even.

01:17:46-01:17:47
But that is prompt engineering.

01:17:47-01:17:49
I don't see that as a job in the future.

01:17:49-01:17:51
What you've described is prompt engineering.

01:17:51-01:17:56
I think I can build that like MCP for myself once.

01:17:56-01:17:57
If that's useful, I'll just put it there.

01:17:57-01:17:59
Completely.

01:17:59-01:17:59
I love that.

01:17:59-01:17:59
That's great.

01:17:59-01:18:02
Or I'll stick it with an ID and give it a nostrum queue

01:18:02-01:18:05
that anybody can integrate.

01:18:05-01:18:09
But I do think it is a super interesting area that we're

01:18:09-01:18:12
moving into with these coding IDs and what you can actually do

01:18:12-01:18:15
with them beyond just building a product.

01:18:15-01:18:18
You can vibe code as a vibe marker.

01:18:18-01:18:19
You can do all of these things.

01:18:19-01:18:21
I have a question for Gav.

01:18:21-01:18:22
I know we're not.

01:18:22-01:18:23
No interities.

01:18:23-01:18:27
I keep thinking I'm not finding a way to slide it in.

01:18:27-01:18:32
So I thought, all right, I'm just going to stop, put my hand up.

01:18:32-01:18:37
But I was wondering, there's a Hemingway statement where

01:18:37-01:18:41
you write drunk and you edit sober.

01:18:41-01:18:46
And the idea is like, this is one of the more intuitive ways

01:18:46-01:18:47
of thinking about it.

01:18:47-01:18:49
All right, there's something about the creative process, which

01:18:49-01:18:51
is two different minds at least.

01:18:51-01:18:53
The person that writes and the person that edits

01:18:53-01:18:55
are not the same sort of person.

01:18:55-01:18:57
And that's one way of pulling it apart.

01:18:57-01:18:59
Don't be curious when you've looked at,

01:18:59-01:19:02
OK, well, creativity is a process.

01:19:02-01:19:04
How would you start to define that process?

01:19:04-01:19:06
What are the different headspaces that you think--

01:19:06-01:19:07
Well, it's true.

01:19:07-01:19:08
It's two different things, right?

01:19:08-01:19:14
So the ideation and the judgment are two separate things.

01:19:14-01:19:18
So you can't to get idea volume.

01:19:18-01:19:20
You can't judge.

01:19:20-01:19:23
Because then you start going, oh, here's an idea.

01:19:23-01:19:25
Oh, hang on, is that a good idea?

01:19:25-01:19:27
And it's almost like context switching.

01:19:27-01:19:28
Yes.

01:19:28-01:19:29
And like--

01:19:29-01:19:31
Yeah, they can't exist in the same space, right?

01:19:31-01:19:35
So you basically-- like that's almost probably

01:19:35-01:19:38
the thing that good creatives can do is just go, well,

01:19:38-01:19:39
I'm just going to come on.

01:19:39-01:19:41
I'm having ideas now.

01:19:41-01:19:44
And we're just going to smash out ideas.

01:19:44-01:19:45
And you just scribble, scribble, scribble,

01:19:45-01:19:48
and something like--

01:19:48-01:19:52
I forget his name now, but our ad lecturer, Kern,

01:19:52-01:19:56
was-- I had this story where--

01:19:56-01:20:01
because like, guys I found kind of get into human psychology

01:20:01-01:20:02
like to--

01:20:02-01:20:04
more to understand their brain, to then be able to do

01:20:04-01:20:08
certain things, to then be able to make it behave differently,

01:20:08-01:20:10
to come up with an idea that no one else would come up with.

01:20:10-01:20:13
And that's where that unique sort of thing comes from,

01:20:13-01:20:18
which is almost like a tweak or adjusting your settings

01:20:18-01:20:23
of an AI or whatever for your volume game.

01:20:23-01:20:29
But so there was a dude who would jump into a pool

01:20:29-01:20:33
with a board and like a waterproof crayon.

01:20:33-01:20:37
And be like, I'm not going up for a breath

01:20:37-01:20:39
until I've got 20 ideas or whatever.

01:20:39-01:20:41
How many ideas?

01:20:41-01:20:43
So it gets to a point where your brain is like,

01:20:43-01:20:46
I need fucking air.

01:20:46-01:20:49
So it starts behaving differently and pulling

01:20:49-01:20:53
all these random connections and all that sort of shit.

01:20:53-01:20:56
And it's just-- I've got my 20 and it goes up.

01:20:56-01:20:56
And he reads it.

01:20:56-01:20:58
It's all about dogs on unicycles.

01:20:58-01:20:58
What the fuck?

01:20:58-01:20:59
Yeah.

01:20:59-01:20:59
Go over there.

01:20:59-01:21:00
Yeah.

01:21:00-01:21:03
And then it becomes a meerkat on a computer.

01:21:03-01:21:06
And a marketer.

01:21:06-01:21:07
But she like that.

01:21:07-01:21:11
So it's like the ideation piece has

01:21:11-01:21:17
to be kept separate to the judgment piece.

01:21:17-01:21:19
I don't know how you'd play that out

01:21:19-01:21:20
if this is where you're going to.

01:21:20-01:21:25
How you'd actually build that into a workflow if you'd need to.

01:21:25-01:21:26
Well, I think--

01:21:26-01:21:28
I always think of these things.

01:21:28-01:21:30
You've got to think about it like you would do with humans.

01:21:30-01:21:31
Yeah.

01:21:31-01:21:31
First.

01:21:31-01:21:33
And you'd say like, because if you

01:21:33-01:21:35
wanted to be creative in one space

01:21:35-01:21:37
and then like critical in another space,

01:21:37-01:21:40
you would literally separate the spaces.

01:21:40-01:21:43
They would not exist in the same space.

01:21:43-01:21:44
You'd have it playing different roles, I reckon.

01:21:44-01:21:45
You might--

01:21:45-01:21:46
You'd have time in between them.

01:21:46-01:21:50
You'd have-- you might even get changed or something or wear

01:21:50-01:21:52
something different to be in different spaces.

01:21:52-01:21:57
Well, that's-- you would get drunk and then sober.

01:21:57-01:22:00
And that's where the-- from a hinting my thing,

01:22:00-01:22:03
like that's-- because obviously your brain is working

01:22:03-01:22:05
differently when you're drunk.

01:22:05-01:22:08
But then you need to like critically adjust and go,

01:22:08-01:22:08
oh, hang on.

01:22:08-01:22:10
This needs to be able to be published,

01:22:10-01:22:11
or this is what, whoever wants, or whatever.

01:22:11-01:22:13
Yeah.

01:22:13-01:22:20
But the-- oh, from an AI perspective

01:22:20-01:22:23
and building that into a workflow,

01:22:23-01:22:27
you'd almost go like, well, what is the type of context

01:22:27-01:22:29
that I need to give to come up with a volume of ideas

01:22:29-01:22:31
for the type of idea that I want to do?

01:22:31-01:22:35
Or who is the good person who's quite good at pulling this

01:22:35-01:22:35
together?

01:22:35-01:22:38
Or what-- like, whatever that would be.

01:22:38-01:22:38
Yeah.

01:22:38-01:22:41
And go like, generate, like however many.

01:22:41-01:22:45
And then you'd have another--

01:22:45-01:22:49
like, you could maybe do like some synth audience stuff

01:22:49-01:22:52
and put it through likes and like,

01:22:52-01:22:54
like artificial focus groups or whatever

01:22:54-01:22:56
of your target audience and all that sort of stuff

01:22:56-01:22:58
to play off the different ideas.

01:22:58-01:23:03
To then refine them down, to then move it on through a flow.

01:23:03-01:23:07
But on the handling my question, like, yeah.

01:23:07-01:23:13
Like, as it is taught, it is like, just pump out

01:23:13-01:23:14
like 1,000 ideas.

01:23:14-01:23:16
And then maybe one or two of them

01:23:16-01:23:19
will be like unique enough to be unpredictable

01:23:19-01:23:20
and all that sort of stuff.

01:23:20-01:23:24
And then you go to like, oh, this one, this one.

01:23:24-01:23:26
Let's like, play those out again.

01:23:26-01:23:28
And then it's like, are they campaignable ideas?

01:23:28-01:23:31
Are they-- what sort of idea are they?

01:23:31-01:23:35
And can you play like, can you put another 100 or 1,000 ideas

01:23:35-01:23:40
off another thread of one of those and that kind of thing?

01:23:40-01:23:46
To then get to your unique creative execution.

01:23:46-01:23:47
It's an interesting--

01:23:47-01:23:48
it's an interesting area to explore.

01:23:48-01:23:50
I mean, we talked about this a little bit last week.

01:23:50-01:23:55
We're talking about there's a distinction between divergent

01:23:55-01:23:57
thinking and convergent thinking.

01:23:57-01:24:00
And like, all the productivity tools all around us,

01:24:00-01:24:04
like, they tend to channel us down more convergent thinking.

01:24:04-01:24:08
But in order to establish the state from which a good idea

01:24:08-01:24:12
can actually emerge, you need that divergent thinking to occur.

01:24:12-01:24:15
And I think this is like, why people

01:24:15-01:24:17
tend to come up with ideas when they're in the bathtub

01:24:17-01:24:19
or having a shower and stuff like that.

01:24:19-01:24:23
You actually need to create the environment for the idea

01:24:23-01:24:25
to natively emerge in the first place.

01:24:25-01:24:28
And it seems to me that outside of those kind of spaces,

01:24:28-01:24:31
which we know of that tend to be quite good for ideation,

01:24:31-01:24:34
seems to me that that's the hard part to try and replicate.

01:24:34-01:24:41
And that's contradicting myself here

01:24:41-01:24:42
on the magic of creativity.

01:24:42-01:24:44
That might be what creatives are good at.

01:24:44-01:24:45
Yeah, OK.

01:24:45-01:24:45
That's what defines them.

01:24:45-01:24:48
It's going to be a process and they're good at the process.

01:24:48-01:24:50
It's just that they don't--

01:24:50-01:24:52
But they know what it is.

01:24:52-01:24:54
Yeah, it's like going into that state of mind.

01:24:54-01:24:56
Or being able to--

01:24:56-01:24:59
it's almost like distracting your monkey mind in a bit,

01:24:59-01:25:04
where it's like, I love listening to podcasts and audiobooks

01:25:04-01:25:07
and that sort of stuff, driving.

01:25:07-01:25:11
Because it's like I'm driving and that's like the--

01:25:11-01:25:15
I don't know, there'd be some psychological name for the type

01:25:15-01:25:15
of focus.

01:25:15-01:25:18
It's like the level of tasks that you're doing it.

01:25:18-01:25:20
But it's not taking up your whole mind.

01:25:20-01:25:21
Yeah, so then--

01:25:21-01:25:22
There's a lot to buy.

01:25:22-01:25:23
There is-- I'm sure there's a word for this.

01:25:23-01:25:25
Is this the thinking process and slow thing from--

01:25:25-01:25:27
like, I don't know, or is it a bit different?

01:25:27-01:25:29
I'd say it'd be a bit different to that.

01:25:29-01:25:30
Yeah, OK.

01:25:30-01:25:31
Thinking--

01:25:31-01:25:32
There's a name for it.

01:25:32-01:25:34
It might be Giz of it in the emphasis.

01:25:34-01:25:37
Basically autopilot, but it's just enough distraction.

01:25:37-01:25:37
Yeah, OK.

01:25:37-01:25:38
Yeah.

01:25:38-01:25:40
It's like listening to a podcast so you could sleep on a plane.

01:25:40-01:25:41
Or something like that.

01:25:41-01:25:43
It's just engaging enough that you're

01:25:43-01:25:45
going to pay attention to it, but not so much.

01:25:45-01:25:45
But your mind can wander.

01:25:45-01:25:46
That you stay awake.

01:25:46-01:25:47
Yeah.

01:25:47-01:25:47
Wonders and Giz of sleep.

01:25:47-01:25:48
Yeah.

01:25:48-01:25:49
Whereas a movie, it's too much.

01:25:49-01:25:51
Right.

01:25:51-01:25:55
So then, like, sort of listening to you--

01:25:55-01:25:56
I can take it in more.

01:25:56-01:25:58
And I'm like, oh, that's a good idea.

01:25:58-01:25:59
That's a good idea.

01:25:59-01:26:00
That's-- I want to come back to that one.

01:26:00-01:26:02
And then how does that play off on that?

01:26:02-01:26:03
You know, all that sort of stuff.

01:26:03-01:26:10
Like that-- yeah, like the getting in--

01:26:10-01:26:12
and that's similar to-- like, they go like--

01:26:12-01:26:14
you've got to sleep on it, which is like your subconscious.

01:26:14-01:26:17
Like, essentially, like, processing it

01:26:17-01:26:18
and all that sort of stuff.

01:26:18-01:26:20
And then you go back to it the next day,

01:26:20-01:26:23
which is like some of the stuff I'm doing at the moment.

01:26:23-01:26:24
I've done that a few times.

01:26:24-01:26:27
And I'm like, yes, I feel like such a sense of progress

01:26:27-01:26:27
on myself.

01:26:27-01:26:28
Like, now I'm at it.

01:26:28-01:26:31
And then the next day, come back and go, what the hell is

01:26:31-01:26:32
that?

01:26:32-01:26:33
[LAUGHTER]

01:26:33-01:26:34
I wonder if there's--

01:26:34-01:26:35
That makes no sense at all.

01:26:35-01:26:37
I wonder if there's a way we can learn

01:26:37-01:26:43
prompt engineering from Charmonds, which is that--

01:26:43-01:26:45
if you think of-- it's not something

01:26:45-01:26:48
that I have yet to have any personal experience with,

01:26:48-01:26:51
maybe, like, once a kid, so that's the way.

01:26:51-01:26:54
But if you think about something talking about like AOSka

01:26:54-01:26:56
or something like that, and like a lot of--

01:26:56-01:26:58
or any psychedelic experience, like a lot of it

01:26:58-01:27:02
seems to go into a set and setting.

01:27:02-01:27:04
Right, and it's almost like they've

01:27:04-01:27:08
like got an approach to prompt and context engineering

01:27:08-01:27:10
prior to entering the state.

01:27:10-01:27:14
I wonder if they'd be quite good at thinking through,

01:27:14-01:27:17
how would you do the set and setting here for this LLM

01:27:17-01:27:21
to come into the world and just say just the right words

01:27:21-01:27:22
and not the wrong words?

01:27:22-01:27:23
Like, what is it?

01:27:23-01:27:24
Has anyone done that?

01:27:24-01:27:24
I don't know.

01:27:24-01:27:26
Like, it just is OK to me.

01:27:26-01:27:29
Got into-- no, not done at AOSka, but like done--

01:27:29-01:27:34
got one of those Charmonds and just sort of like--

01:27:34-01:27:35
there'd have to be a--

01:27:35-01:27:37
Charmonds 2.0.

01:27:37-01:27:39
Yeah, there'd have to be that sort of thing done.

01:27:39-01:27:41
And then what the context--

01:27:41-01:27:43
like, that's almost like training a whole new model.

01:27:43-01:27:44
Yeah.

01:27:44-01:27:45
I mean, they're different.

01:27:45-01:27:47
Like, there's definitely like a methodology to it, right?

01:27:47-01:27:50
Because they do this with a lot of the scientific research

01:27:50-01:27:53
that they've been doing into various like psilocybin

01:27:53-01:27:54
and MDMA and stuff.

01:27:54-01:27:56
But a lot of it comes down to, all right,

01:27:56-01:27:57
well, you've got to make sure--

01:27:57-01:27:59
like, it's seen as important that the set and setting

01:27:59-01:28:00
are correct, therefore.

01:28:00-01:28:01
Yeah.

01:28:01-01:28:03
There'll have been documented evidence

01:28:03-01:28:04
of how you approach that.

01:28:04-01:28:06
I wonder if there is just like a little bit of--

01:28:06-01:28:09
it's again, it's not a world I know much about.

01:28:09-01:28:12
But you wonder if there's a bit of a methodology for the person

01:28:12-01:28:15
that's good at that probably knows how to be good at it.

01:28:15-01:28:17
Like, is there something we can learn from that person

01:28:17-01:28:18
about how we--

01:28:18-01:28:20
because you almost, in a way, like, you

01:28:20-01:28:23
birth these LLMs into the world, these agents

01:28:23-01:28:26
into the world for like 20 seconds.

01:28:26-01:28:28
Then they do a task, and then they go away.

01:28:28-01:28:29
And we're saying, like, look, just be born.

01:28:29-01:28:32
And then just be really good at stuff.

01:28:32-01:28:34
Fuck off, and never exist again.

01:28:34-01:28:35
And you kind of-- like, there's got

01:28:35-01:28:37
to be a way of like, enchanting them

01:28:37-01:28:40
like with these weird spells that we put into it to say--

01:28:40-01:28:42
And then they have vaccinations and stuff.

01:28:42-01:28:43
So you go more.

01:28:43-01:28:45
So you only know the right stuff when you come in.

01:28:45-01:28:47
You only behave in the way that you should be.

01:28:47-01:28:49
And there probably is like a way of--

01:28:49-01:28:51
like, maybe it's all in like occult books

01:28:51-01:28:53
around summoning demons.

01:28:53-01:28:56
It's actually just got mis-translated over the years.

01:28:56-01:28:58
And it was actually all about context engineering.

01:28:58-01:28:58
Yeah.

01:28:58-01:29:00
Well, so summon LLMs.

01:29:00-01:29:05
So summon your LLMs before the sky god knocked them out.

01:29:05-01:29:07
Because as you're saying that, I'm thinking, like, you know,

01:29:07-01:29:09
like, the--

01:29:09-01:29:12
was it that, the good father of AI dude?

01:29:12-01:29:13
Definitely not the perfect one.

01:29:13-01:29:15
Yeah, he's all about, like, is--

01:29:15-01:29:18
to almost like the risk voice and the warning voice.

01:29:18-01:29:20
It's like, oh, this is going to annihilate humans,

01:29:20-01:29:21
all this sort of stuff.

01:29:21-01:29:23
But it's like, it's--

01:29:23-01:29:23
So it's everything.

01:29:23-01:29:24
I mean, we're living--

01:29:24-01:29:27
But it's that just because we're all like,

01:29:27-01:29:29
shamaning this thing.

01:29:29-01:29:31
And we're just giving it a bad trip.

01:29:31-01:29:32
And all of a sudden--

01:29:32-01:29:32
Exactly.

01:29:32-01:29:33
You've found a good trap.

01:29:33-01:29:38
I've been listening to a bit of like--

01:29:38-01:29:38
It's an interesting--

01:29:38-01:29:39
--Great Hang-On.

01:29:39-01:29:39
--And Joe Rogan.

01:29:39-01:29:40
Yeah, I could tell.

01:29:40-01:29:40
Yeah.

01:29:40-01:29:42
[INAUDIBLE]

01:29:42-01:29:44
There were some threads that were coming up there.

01:29:44-01:29:45
It's a sandwich familiar.

01:29:45-01:29:46
Oh, look, it's definitely like--

01:29:46-01:29:47
It's interesting.

01:29:47-01:29:49
--the sort of thing that I'm quite interested in.

01:29:49-01:29:50
Yeah.

01:29:50-01:29:52
But as I said, it's just, life-wise,

01:29:52-01:29:53
this is not the time.

01:29:53-01:29:54
Have you read any of his books?

01:29:54-01:29:55
Hang-On 2.

01:29:55-01:29:56
I am reading--

01:29:56-01:29:58
Great, I got fingerprints of the dogs.

01:29:58-01:29:59
That was the first one.

01:29:59-01:30:00
It's a good one.

01:30:00-01:30:01
Yeah.

01:30:01-01:30:03
What's your reading speed?

01:30:03-01:30:04
My reading speed?

01:30:04-01:30:05
1.

01:30:05-01:30:05
Oh, yeah.

01:30:05-01:30:07
I was reading 1x.

01:30:07-01:30:10
I thought-- no, I think it varies, actually, depending

01:30:10-01:30:10
on what I'm reading.

01:30:10-01:30:12
Go ahead and estimate it.

01:30:12-01:30:13
You're smart.

01:30:13-01:30:15
How many--

01:30:15-01:30:16
I don't know.

01:30:16-01:30:17
I read pretty quick.

01:30:17-01:30:17
Yeah.

01:30:17-01:30:18
Yeah.

01:30:18-01:30:19
But it does depend--

01:30:19-01:30:20
Anyone can use a look.

01:30:20-01:30:21
It has to be really quick.

01:30:21-01:30:22
He's still competitive.

01:30:22-01:30:23
But I read pretty quick.

01:30:23-01:30:23
I don't know.

01:30:23-01:30:24
Pretty quick, but it's a huge--

01:30:24-01:30:25
But it depends.

01:30:25-01:30:26
Don't give him any keys.

01:30:26-01:30:27
Yeah.

01:30:27-01:30:28
No, but it depends, right?

01:30:28-01:30:29
Like, I think if there's--

01:30:29-01:30:31
Because essentially, like, your download speed, right?

01:30:31-01:30:35
It's like, how quickly can you absorb information?

01:30:35-01:30:37
It's an interesting question, because I actually

01:30:37-01:30:39
think it is highly subjective.

01:30:39-01:30:40
Like, when I'm reading a knob--

01:30:40-01:30:44
like, if I'm reading fiction, I read it super fast.

01:30:44-01:30:51
I couldn't even try to guess it.

01:30:51-01:30:51
But I guess--

01:30:51-01:30:53
Do you just read, or do you read with an audiobook

01:30:53-01:30:54
overlay to the thing?

01:30:54-01:30:55
No, I just read.

01:30:55-01:30:58
Because that's like a trick for speed reading.

01:30:58-01:31:00
You take an audiobook, and you speed the audiobook up,

01:31:00-01:31:03
and then you track with your finger along the words,

01:31:03-01:31:04
and you do the whole thing.

01:31:04-01:31:06
And there's like, software you can use to then sync

01:31:06-01:31:09
like the book and the audio for you.

01:31:09-01:31:10
I've got a bad habit of--

01:31:10-01:31:13
Like, if I know the person who's written the book,

01:31:13-01:31:16
and I know what they sound like, I will start to read it

01:31:16-01:31:19
in their voice, and that will slow down my reading.

01:31:19-01:31:22
I get so annoyed when I've read a bunch of books.

01:31:22-01:31:24
Prides are then finding out there's a film or something,

01:31:24-01:31:25
and the person's got the wrong voice.

01:31:25-01:31:26
Or even in--

01:31:26-01:31:27
Disappointed.

01:31:27-01:31:30
Where, like, early on in the book, I would have, like,

01:31:30-01:31:32
invented-- like, as soon as I see a name,

01:31:32-01:31:35
it's like, I've never pictured this person in their clothes

01:31:35-01:31:37
and everything else that's going on.

01:31:37-01:31:39
Just wanted to see if we wanted anything else.

01:31:39-01:31:41
Not-- we're good.

01:31:41-01:31:46
Yeah, so I would see this, like, the person in these clothes

01:31:46-01:31:47
and this face.

01:31:47-01:31:48
This is what they look, and their hair and everything.

01:31:48-01:31:51
And then, like, a chapter on it would tell me, like, oh, no,

01:31:51-01:31:53
this person's got, like, blonde hair.

01:31:53-01:31:55
And I'm like, no, no, no, no, no.

01:31:55-01:31:57
I've invested, like, two chapters into this.

01:31:57-01:32:00
And this person looks exactly like this.

01:32:00-01:32:03
This is like-- this was everyone's reaction to Tom Cruise

01:32:03-01:32:05
being cast as Jack Reacher.

01:32:05-01:32:06
I mean, that was ridiculous.

01:32:06-01:32:08
I mean, they've got good Jack Reacher now.

01:32:08-01:32:09
It looks like--

01:32:09-01:32:10
In the TV show?

01:32:10-01:32:10
Yeah, he's like a Jack Reacher.

01:32:10-01:32:13
He's huge.

01:32:13-01:32:15
Yeah, Tom Cruise is very ridiculous with that role.

01:32:15-01:32:17
This was a tangent I wasn't expecting.

01:32:17-01:32:20
But to be honest, I never know what to expect from a podcast.

01:32:20-01:32:21
I just go all over the place.

01:32:21-01:32:23
But yeah, the grey man comes to like--

01:32:23-01:32:24
--sharming.

01:32:24-01:32:25
Hello, and--

01:32:25-01:32:27
We have, like, seen Tom Cruise.

01:32:27-01:32:28
I think it does, yeah.

01:32:28-01:32:29
--what I think about it.

01:32:29-01:32:31
There's maybe a good way of explaining to it.

01:32:31-01:32:32
It's like, they need--

01:32:32-01:32:33
they need to be birthed.

01:32:33-01:32:35
It is, like, summoning demons.

01:32:35-01:32:36
Yeah.

01:32:36-01:32:37
It's going to be--

01:32:37-01:32:41
that'll be the upskilling thing that's good for things to do

01:32:41-01:32:41
now.

01:32:41-01:32:44
And that, like, going back to the strict directory

01:32:44-01:32:48
versus Google Maps thing, that will

01:32:48-01:32:52
be a tell on the people who can adapt to it versus those

01:32:52-01:32:53
who can't.

01:32:53-01:32:56
Some people just won't be able to figure out that they need

01:32:56-01:32:57
to go, well, hang on.

01:32:57-01:32:57
Oh, hang on.

01:32:57-01:32:59
I don't just do the thing now.

01:32:59-01:33:03
I have to sort of communicate or interpret how to do it

01:33:03-01:33:04
to something.

01:33:04-01:33:06
Let something else do the thing.

01:33:06-01:33:08
Product name, spellbook.

01:33:08-01:33:08
Spellbook.

01:33:08-01:33:10
That MCP server that makes your prompts better.

01:33:10-01:33:11
Oh, that's good.

01:33:11-01:33:13
It gives you the incantations.

01:33:13-01:33:14
Or incantation.

01:33:14-01:33:15
Incantations nice as well.

01:33:15-01:33:16
Yeah.

01:33:16-01:33:17
It's recorded now.

01:33:17-01:33:18
Possession.

01:33:18-01:33:18
Possession.

01:33:18-01:33:19
Something like that.

01:33:19-01:33:20
We've got them.

01:33:20-01:33:20
Potion.

01:33:20-01:33:21
Potion.

01:33:21-01:33:21
Yeah.

01:33:21-01:33:22
Like, no judgment here.

01:33:22-01:33:23
We're in creative.

01:33:23-01:33:24
Yeah, we're in--

01:33:24-01:33:24
This is it, yeah.

01:33:24-01:33:25
We're drunk.

01:33:25-01:33:28
But you know, like, to come back to the idea piece,

01:33:28-01:33:29
like, I actually--

01:33:29-01:33:30
like, I tend to think that in order

01:33:30-01:33:32
to generate those really insightful ideas,

01:33:32-01:33:35
you actually need to step away from all of the technology.

01:33:35-01:33:37
You know, because it's like you're harvesting,

01:33:37-01:33:38
like, experiences.

01:33:38-01:33:41
And the experiences are shaping how you think.

01:33:41-01:33:43
And they're creating these connections.

01:33:43-01:33:46
And those connections lead to different experiences

01:33:46-01:33:47
and different outputs and inputs.

01:33:47-01:33:49
And it is, like, the qualia point

01:33:49-01:33:50
that Pete was making earlier.

01:33:50-01:33:53
It's like, you can't really replicate that with a model.

01:33:53-01:33:55
It's like, last week we were talking about--

01:33:55-01:33:58
I gave this example at Rory Sutherland, you know,

01:33:58-01:34:00
the guy from Ogilvie.

01:34:00-01:34:02
You'd be familiar with him, I imagine.

01:34:02-01:34:05
He gives this example of, like, you know,

01:34:05-01:34:06
they had in their office building,

01:34:06-01:34:09
they had-- everyone was complaining about how slow

01:34:09-01:34:10
the elevators were.

01:34:10-01:34:13
And so they were like, OK, well, we

01:34:13-01:34:15
could spend a million dollars to try and speed these things up

01:34:15-01:34:17
by 5%.

01:34:17-01:34:21
Or we could just spend $100 and put mirrors

01:34:21-01:34:22
everywhere in the lift.

01:34:22-01:34:24
And that solved the problem.

01:34:24-01:34:26
Because now everyone was, like, preoccupied

01:34:26-01:34:28
with, like, watching other people and also their own appearance

01:34:28-01:34:30
and that kept themselves busy.

01:34:30-01:34:32
And they got-- they basically distracted while they were

01:34:32-01:34:33
in the lift.

01:34:33-01:34:37
And it's like that kind of, like, creative thinking.

01:34:37-01:34:38
Like, there's a mechanical solution.

01:34:38-01:34:40
And there's, like, a really creative solution.

01:34:40-01:34:43
Going back to the slight perception management,

01:34:43-01:34:47
that essentially, but the--

01:34:47-01:34:50
like, how the human nature tell, like,

01:34:50-01:34:53
you're getting to a point that you're complaining about the lift

01:34:53-01:34:54
being too slow.

01:34:54-01:34:55
Too slow.

01:34:55-01:34:57
Like, yeah, let's say the stairs, man.

01:34:57-01:34:59
Yeah, but that's even slower.

01:34:59-01:35:01
The slightly quicker lift is slightly better.

01:35:01-01:35:02
Yeah.

01:35:02-01:35:05
But whatever you use the lift in this building, it's not quick.

01:35:05-01:35:07
But it's something-- like, there's--

01:35:07-01:35:09
no matter what, people find something you complain about.

01:35:09-01:35:13
Or that they can't tell on the people versus--

01:35:13-01:35:14
Yeah.

01:35:14-01:35:14
This is--

01:35:14-01:35:17
It is more just, like, the creative application

01:35:17-01:35:20
to solving for that, which is, like, you could have gone down,

01:35:20-01:35:24
though, like, a really, like, kind of more productive,

01:35:24-01:35:26
engineering-based approach to solving that problem,

01:35:26-01:35:28
which would result in, like, trying to optimize the lift

01:35:28-01:35:30
speeds by 5%.

01:35:30-01:35:32
And you'd spend a shitload of money doing that.

01:35:32-01:35:33
He's got so many good examples.

01:35:33-01:35:35
And the other one I think we talked about was--

01:35:35-01:35:37
like, they talked about the Euro--

01:35:37-01:35:39
the EuroStar being too slow.

01:35:39-01:35:41
They said, right, well, OK, speeding up the train

01:35:41-01:35:43
could cost you, like, hundreds of millions, billions of dollars.

01:35:43-01:35:47
Or we just get some hot female and male models,

01:35:47-01:35:48
and they serve everyone champagne,

01:35:48-01:35:50
and that costs 100 grand.

01:35:50-01:35:51
Job done.

01:35:51-01:35:54
They'll beg us to slow the train down.

01:35:54-01:35:55
That's true.

01:35:55-01:35:57
It's like identifying what the real problem is.

01:35:57-01:35:57
Exactly.

01:35:57-01:36:00
It's like, look, the problem is that it's something else.

01:36:00-01:36:04
It's like, there's some sort of unmet need psychologically

01:36:04-01:36:05
that's going on, which is exerting yourself as, oh,

01:36:05-01:36:06
this is too slow.

01:36:06-01:36:07
Yes.

01:36:07-01:36:07
I need to get there.

01:36:07-01:36:09
It's like, well, you could have just left it.

01:36:09-01:36:09
That's like--

01:36:09-01:36:10
It's probably quite fast enough.

01:36:10-01:36:14
Is it worth that much money to go faster,

01:36:14-01:36:16
or if you're more comfortable, does the whole thing go away?

01:36:16-01:36:17
Yeah.

01:36:17-01:36:21
It's like-- it's like a brand-researching thing,

01:36:21-01:36:26
is you don't actually ask customers what they want,

01:36:26-01:36:27
because they don't know what they want.

01:36:27-01:36:31
Or they'll tell you something, or they're past year three,

01:36:31-01:36:33
so their brains are thinking about what the answer is

01:36:33-01:36:35
that they should be giving or what you want to hear

01:36:35-01:36:36
and all that sort of shit.

01:36:36-01:36:37
It's the faster ones.

01:36:37-01:36:38
So you've actually got it-- yeah, you've

01:36:38-01:36:41
got to actually set up your questionnaires

01:36:41-01:36:42
and all that to capture the information,

01:36:42-01:36:47
to then be able to correlate back what it was that they actually

01:36:47-01:36:48
won.

01:36:48-01:36:50
It feels to me like you game that as well.

01:36:50-01:36:51
I mean, that's what political surveys are, right?

01:36:51-01:36:54
Like the way you structure a series of questions in a survey,

01:36:54-01:36:56
you can essentially game out the outcome that you

01:36:56-01:36:58
want from the survey.

01:36:58-01:36:59
Yeah.

01:36:59-01:36:59
It's so--

01:36:59-01:37:01
If you've got the motive, then--

01:37:01-01:37:03
Yeah, that's the problem, right?

01:37:03-01:37:06
It's like, vested interest is always

01:37:06-01:37:08
in some way baked into a lot of this stuff.

01:37:08-01:37:13
The-- on a brand side, though, you want to eliminate that,

01:37:13-01:37:16
because otherwise, you're not getting--

01:37:16-01:37:16
It's not representing.

01:37:16-01:37:19
Yeah, like you're not getting your market orientation.

01:37:19-01:37:24
You're not understanding what is out there,

01:37:24-01:37:26
so then you can actually, oh, this

01:37:26-01:37:28
is actually what's going on.

01:37:28-01:37:29
How can we change what we're doing,

01:37:29-01:37:30
or where do we want to do it?

01:37:30-01:37:34
If you're sort of manipulating the data--

01:37:34-01:37:37
I suppose it depends on whether you're actually

01:37:37-01:37:39
the company who wants to know, or if you're

01:37:39-01:37:42
the company that's been hired by the company that want--

01:37:42-01:37:45
and then, yeah.

01:37:45-01:37:47
Because sometimes, this is just people working with people.

01:37:47-01:37:50
Sometimes you hire someone to just hold a mirror back up

01:37:50-01:37:52
to your own points of view.

01:37:52-01:37:52
Oh, man, that is so--

01:37:52-01:37:54
That's a problem.

01:37:54-01:37:55
Yeah.

01:37:55-01:37:57
I used to hate getting sales dudes who just like,

01:37:57-01:38:00
ah, they ask the other questions, and then they present them.

01:38:00-01:38:01
Like, dude, I already know that.

01:38:01-01:38:03
That's exactly what I told you.

01:38:03-01:38:04
Like, you haven't told me anything.

01:38:04-01:38:05
You just wasted my fucking time.

01:38:05-01:38:06
Thanks.

01:38:06-01:38:08
So yeah.

01:38:08-01:38:10
You hired me to tell you back.

01:38:10-01:38:11
We just--

01:38:11-01:38:12
Yeah.

01:38:12-01:38:15
Well, it's like, you see that a lot with consulting, right?

01:38:15-01:38:17
It's an insurance policy more than anything else.

01:38:17-01:38:20
It sits in the top drawer, and if anything goes wrong,

01:38:20-01:38:21
we've got someone to blame.

01:38:21-01:38:24
It's like plausible deniability at that point.

01:38:24-01:38:25
It seems to work.

01:38:25-01:38:28
It's expensive, but it seems to work.

01:38:28-01:38:29
How are we going on time?

01:38:29-01:38:30
I think we're probably-- we're always

01:38:30-01:38:31
going to get kicked out of here, I think.

01:38:31-01:38:33
I think so, yeah.

01:38:33-01:38:35
All right, well, I mean, with that in mind--

01:38:35-01:38:37
Not a bad place to rap, I think.

01:38:37-01:38:38
Well, what else have you got, Gab?

01:38:38-01:38:40
What's your sign off?

01:38:40-01:38:42
What's my sign off?

01:38:42-01:38:45
What's your favorite bit of the discussion?

01:38:45-01:38:50
I quite liked your shamaning.

01:38:50-01:38:51
Yeah.

01:38:51-01:38:51
That was new.

01:38:51-01:38:52
That was very new for--

01:38:52-01:38:53
It was good.

01:38:53-01:38:58
Because it's almost like layering in pre--

01:38:58-01:39:01
like go pre-mod of medicine sort of stuff,

01:39:01-01:39:04
and you feel like Chinese medicine, all that sort of stuff.

01:39:04-01:39:09
And it's like the clash between those

01:39:09-01:39:12
applied to AI now versus--

01:39:12-01:39:15
well, like current life now versus future AI life.

01:39:15-01:39:16
Yeah.

01:39:16-01:39:18
All that plays out.

01:39:18-01:39:20
And then you drop that down into--

01:39:20-01:39:23
because you become a stratification of society

01:39:23-01:39:27
and schooling and all that sort of stuff.

01:39:27-01:39:32
I don't know.

01:39:32-01:39:34
I've never signed off a podcast before.

01:39:34-01:39:35
That's the good stuff.

01:39:35-01:39:36
Don't you?

